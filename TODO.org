* TODO cascalag.contrib
  From Aaron:

  #+BEGIN_QUOTE
  Props to Chun for pointing this out. Has some striking parallels to
  some of our requirements, so maybe a great source of inspiration for
  syntax. http://sritchie.github.com/2011/11/15/introducing-cascalogcontrib.html
                                                                                                                                                                                                                                                                                                                                                                                                                                         
  Notice for example there's an implicit naming convention for
  specifying sub tasks that run in parallel, vs. in series.
  #+END_QUOTE
* TODO [[http://hadoop.apache.org/common/docs/stable/mapred_tutorial.html][MapReduce tutorial]]
  When loading data, load into =DistributedCache=; [[https://github.com/stuartsierra/clojure-hadoop][clojure-hadoop]].
* TODO Attach-geocode POC
  #+BEGIN_EXAMPLE
    geocode-data(i3): /apps/extract/poi/UnitedKingdomScarecrow/input/geocode_data
    md5-uuid-mapping(i2): /apps/extract/poi/UnitedKingdomScarecrow/output/split_test/014_compute_uuids
    deduped_entities(i1): /apps/extract/poi/UnitedKingdomScarecrow/output/split_test/020_combined_deduped_and_validation_data
  #+END_EXAMPLE

  I.e. [[http://d22.factual.com.:50075/browseDirectory.jsp?dir=%2Fapps%2Fextract%2Fpoi%2FUnitedKingdomScarecrow%2Finput%2Fgeocode_data&namenodeInfoPort=50070&delegation=null][geocode_data]], [[http://d22.factual.com.:50075/browseDirectory.jsp?dir=%2Fapps%2Fextract%2Fpoi%2FUnitedKingdomScarecrow%2Foutput%2Fsplit_test%2F014_compute_uuids&namenodeInfoPort=50070&delegation=null][compute_uuids]], [[http://d22.factual.com.:50075/browseDirectory.jsp?dir=%2Fapps%2Fextract%2Fpoi%2FUnitedKingdomScarecrow%2Foutput%2Fsplit_test%2F022_combined_deduped_and_validation_and_geocoding_data&namenodeInfoPort=50070&delegation=null][combined_deduped_and_validation_data]].

  [[https://github.com/Factual/back/blob/master/datastore-objects/src/main/thrift/factual_data_objects.thrift][Thrift-spec]]: payload and payloadRaw are JSON; [[https://github.com/Factual/back/blob/master/datastore-objects/src/main/java/com/factual/adaptors/Input.java][Wraps the input-data
  object]]: i.e. parses the JSON, provides a map.

  "Attach": append it to the array data-objects; eventually:
  summarization merges the array of data-objects. Rules: mode, mean;
  more complex rules, e.g. this came from that source and has a higher
  score.

  Tab-delimited data; sequence files: key-type, value-type.

  uuid -> data; md5 -> uuid; md5 -> geodata; [[http://d11.factual.com:50075/browseBlock.jsp?blockId=-901183859042176514&blockSize=30109191&genstamp=13911775&filename=%2Fapps%2Fextract%2Fpoi%2FUnitedKingdomScarecrow%2Finput%2Fgeocode_data%2Fgeocode_data_2010_07_24&datanodePort=50010&namenodeInfoPort=50070&delegation=null][geodata]]. Represent the
  data as JSON; propagate geodata back.

  Generalized attachment: join, attach, summarize.

  - Task 1 (non-Hadoop)
    - Copy data to HDFS: comma-delimited md5-i -> uuid-i.
    - Input :: local file
    - Output :: md5->uuid (comma delimited)
  - Task 2 (cascalog?)
    - Convert output of task 1 to tab-delimited md5 -> uuid-i.
    - Input :: md5->uuid (comma delimited)
    - Output :: md5->uuid (tab delimited)
  - Task 3 (straight-up Java?)
    - Join task 2 with md5-i -> data-i, such that uuid-i -> data-i
      (tab delimited).
    - Input :: md5->uuid (tab delimited)
    - Output :: uuid->data (tab delimited)

  Simulates: non-Hadoop, Hadoop-transformation,
  Hadoop-join. (Cf. [[http://hadoop.apache.org/common/docs/stable/mapred_tutorial.html][this]], by the way.)
* TODO Spec
  If we have:

  #+BEGIN_SRC clojure
    (deftask b
      :children (c d e)
      :dependencies (a))
  #+END_SRC

  I also want:

  #+BEGIN_SRC clojure
    (deftask a ...)
    (deftask b ...)
    (deftask c ...)
    ...
    
    (make-task-tree!
     (a
      (b
       (c d e))))
    
    (make-dependency-tree!
     (a
      (b)))
  #+END_SRC

  where tasks are created with the default settings, if they don't
  exist; possibly with a warning on stdout.
* TODO Proof-of-concept
  Chain two map-reduce tasks together. Vineyardize the tasks (without
  napa).

  Bogus wordcount example?

  Capitalize, count.

  Over hadoop.

  Non-hadoop precondition: moves local file with noisy words to HDFS;
  in hadoop: normalization (upper-case) and count.

  Output: word to count mapping:

  #+BEGIN_EXAMPLE
    ASS 1
    DONKEY 10
  #+END_EXAMPLE

  Validation: validating counters (name of counter, value),
  hdfs-file-exists?, hdfs-file-empty?

  #+BEGIN_SRC clojure
    (defn hadoop-counter [counter-name]
      ...)
    
    (defn call-with-hadoop-conditions [f]
      (f *hadoop-conditions*))
    
    (defn non-zero-hadoop-conditions? []
      (call-with-hadoop-conditions
        (fn [hadoop-conditions]
          (> (count hadoop-conditions) 0))))
    
    (if (non-zero-hadoop-conditions?)
      (throw ...Exception))
    
    (defn get-hadoop-job [vineyard-task]
      (...))
    
    (defn get-hadoop-counter [vineyard-task counter-name]
      (...))
    
    (defn get-hadoop-property [vineyard-ask property-name]
      (...))
    
    (> (get-hadoop-counter *vineyard-task* "foo") 0)
    
    (defn hdfs-file-exists? [vineyard-task path]
      ;; Check for the existence of _SUCCESS.
      (...))
    
    (hdfs-file-exists? *vineyard-task* "/path/to/dedupe")
    
    ;;; Inside pre-dedupe-analysis; path defaults to "/path/to/dedupe". In
    ;;; other words, "does the default input path of my parent exist?"
    (hdfs-file-parent-exists? *vineyard-task*)
    
  #+END_SRC

  [[http://wiki.corp.factual.com/display/INFRA/Vineyard+Java+Driver][Vineyard Java client]]. MapReduce jobs in Clojure? And pre-existing
  code in Java.

* TODO Extract POC
  [[https://github.com/Factual/hadoop-extraction-workflow/blob/master/src/java/workflows/extract/poi/UnitedStatesExtraction.java][US-extraction]]; enumerated subtasks:

  #+BEGIN_SRC java
    List<mapreduce.Task> tasks =
        Lists.newArrayList
        (
         writeHeaders,
         computeUniqueInputs,
         convertGeocodingResultsToTab,
         convertValidationJsonResultsToTab,
         writeUuidRetentionMappingToSequenceFile,
         extractEntities,
         computeSortedUniqueMd5s,
         analyzeExtractedEntities,
         postProcessExtractedData,
         preDedupeAnalysis,
         preDedupeAnalysisSummary,
         generateLikelyDupeMd5s,
         uniquifyLikelyDupeMd5s,
         computeDedupeUuids,
         assignUuids,
         groupDedupedEntities,
         dedupeQA,
         removeJunkInputsAndEntities,
         assignUuidsToValidationResults,
         combineDedupedAndValidationData,
         assignUuidsToGeocodingResults,
         combineDedupedAndGeocodingData,
         performFinalPostprocessing,
    
         computeUuidRetentionMapping,
         removeOverfoldingRetainedUuids,
         applyUuidRetentionMapping,
    
         exportData,
         exportDataQA,
    
         uuidRetentionTracker
         );
    
  #+END_SRC

  [[http://d22.factual.com.:50075/browseDirectory.jsp?dir=%2Fapps%2Fextract%2Fpoi%2FUnitedKingdomScarecrow%2Foutput%2Fleo_uuid_test&namenodeInfoPort=50070&delegation=null][Output]].

  #+BEGIN_SRC sh
    sudo hadoop jar hadoop-extraction-workflow-hadoop.jar \
        workflows.extract.poi.UnitedKingdomExtraction \
        hadoop_config_file=conf/mapreduce/MapReduceRunner/n_cluster.properties \
        extraction_config_class=extract.poi.UnitedKingdomScarecrow \
        project_name=UK_scarecrow_extraction_test \
        extraction_dataset_id=G4YzkQ \
        summary_view_id=cZqm0N
  #+END_SRC

  Modules:

  - Extract
  - Dedupe
  - Attach geo
  - Attach validation
  - UUID retention

  #+BEGIN_SRC lisp
    ;;; Grouping
    (hadoop-extraction-workflow
     (extract
      ^{predecessors: (extract)}
      (write-headers
       compute-unique-inputs
       convert-geocoding-results-to-tab
       convert-validation-json-results-to-tab
       write-uuid-retention-mapping-to-sequence-file
       extract-entities
       compute-sorted-unique-md5s
       analyze-extracted-entities
       post-process-extracted-data))
     (dedupe
      (pre-dedupe-analysis
       pre-dedupe-analysis-summary
       generate-likely-dupe-md5s
       uniquify-likely-dupe-md5s
       compute-dedupe-uuids
       assign-uuids
       group-deduped-entities
       dedupe-qa
       remove-junk-inputs-and-entities
       perform-final-postprocessing))
     (attach-geo
      (assign-uuids-to-geocoding-results
       combine-deduped-and-geocoding-data))
     (attach-validation
      (assign-uuids-to-validation-results
       combine-dedupe-and-validation-data))
     (uuid-retention
      (compute-uuid-retention-mapping
       remove-overfolding-retained-uuids
       apply-uuid-retention-mapping
       export-data
       export-data-qa
       uuid-retention-tracker)))
    
    ;;; Precedence
    (dedupe (extract))
    
    ;;; Gantt charts
    
    (a
     (b
      (c d)))
    
  #+END_SRC

* TODO =yaml= to vineyard
  We're going to have a =.onStart=, =.onFinish=; yaml leaves specify
  tasks. Have a predecessor thing:

  #+BEGIN_EXAMPLE
    iris
      chrome plugin
      nlp
    api
      places data
      sugar
    demo
      webui (depends 1, 4)
  #+END_EXAMPLE
* Notes
** Mon Dec 19 16:50:50 PST 2011   
   - at the end of task: check succeeded (Vineyard task); it fails;
     responsibility of the vineyard task to fail;
   - takes YAML: turns into command-line options
   - napa is the consumer that can run in daemon or cli mode (latter:
     takes yaml file, presents
   - name of the yaml file, config-argument
   - folders of yaml files
   - yaml files exist in scarecrow?
   - yaml files served up by screws?
   - composition of yaml-files?
   - "this step is actually this file"

