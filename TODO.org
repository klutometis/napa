* TODO Job-spec
  Two functions: a =defjob= which takes a symbol, qualified class name
  and optional metadata (should be exported into a namespace for
  composability); =set-dependencies!= which takes a dependency graph
  of jobs.

  #+BEGIN_SRC clojure
    (load-file "othertoy.clj")
    
    (defn def-task [class init-data]
      (doto (new class)
        (set-init-data)))
    
    (defn def-hadoop-task []
      (def-task MapReduceTask {:blah "blah"}))
    
    (defn def-shell-task [command]
      )
    
    
    (deftask make-uuid-md5s hewtoy/UUIDMD5Maker)
    
    (deftask ^{:dependencies [make-uuid-md5s]
               :input-path "some-input-path"
               :task-type hadoop}
      uuid-to-payload-mapper hewtoy/UUIDToPayloadMapper)
    
    (deftask ^{:dependencies [make-uuid-md5s]}
      uuid-to-md5-mapper hewtoy/UUIDToMD5Mapper)
    
    (deftask md5-and-payload-joiner hewtoy/MD5AndPayloadJoiner)
    
    (deftask ls-tmp hewtoy/NewLS)
    
    (set-dependencies!
     (md5-and-payload-joiner
      ((uuid-to-md5-mapper
        make-uuid-md5s)
       (uuid-to-payload-mapper
        make-uuid-md5s
        randomtask))))
    
    (set-root! md5-and-payload-joiner)
    
  #+END_SRC

  Build out the root; send the root to vineyard. Have hadoop consumers
  already running.

  For the time being, comma delimited list: "input_paths"; also:
  java-interop;

  Draw dependency graphs: napa or vineyard?

  We need to distinguish between different classes of tasks: there's
  vanilla Vineyard tasks, but also shell-tasks, hadoop-tasks.

  General pattern: instantiate some nullary constructor; specify
  initData.

  Clojure driver based on the [[https://github.com/Factual/vineyard-java-driver][Java driver]].

  Two axes: dependency vs. predecessorship; expressed independently,
  too. See [[http://en.wikipedia.org/wiki/Tree_(graph_theory)#Definitions][this]], by the way:

  #+BEGIN_QUOTE
  An ordered tree or plane tree is a rooted tree for which an ordering
  is specified for the children of each vertex.
  #+END_QUOTE

  Also:

  #+BEGIN_QUOTE
  The term hedge sometimes refers to an ordered sequence of trees.
  #+END_QUOTE

  Are we dealing with a partially ordered tree? Where ordering
  expresses synchronicity (sequence).

  Generic =make-task= interface with a class and an init-data map;
  gets specialized into =make-hadoop-task= (or, alternatively,
  =task=), =make-shell-task= (=shell-task=), &c.

  =task=, =hadoop-task= (=map-reduce-task=, for that matter),
  =shell-task= are good if we're going declaritive; =dependency-graph=
  (or simply =dependencies=), furthermore; as well as
  =predecessor-graph= (or simply =predecessors=).[fn:: We'd only need
  to specify =-graph= is there was some meaningful distinction, such
  as e.g. =-tree=?] (How to visually express dependency and
  predecessorship in the same graph, by the way: differently colored
  links? Two graphs? Numbers?)

  Also, the Clojure metadata stuff is pretty ugly; different mechanism
  for specifying the init-data?

  See [[http://en.wikipedia.org/wiki/Dataflow_programming][dataflow-programming]]; [[http://stackoverflow.com/questions/4565158/using-clojure-dataflow-programming-idioms][in Clojure]]; [[http://richhickey.github.com/clojure-contrib/dataflow-api.html][API]]. [[http://upload.wikimedia.org/wikipedia/en/3/33/FBP_3_block_diagram.jpg][Block diagramm]].

  [[http://ditaa.sourceforge.net/][ditaa]] is what I'm talking about: parse it and input it. [[https://github.com/stathissideris/ditaa][Source]]. Take
  a look at [[https://github.com/stathissideris/ditaa/blob/master/src/org/stathissideris/ascii2image/core/CommandLineConverter.java#L222][this]]; basically:

  #+BEGIN_SRC java
    TextGrid grid = new TextGrid();
    grid.loadFrom(fromFilename, options.processingOptions);
    Diagram diagram = new Diagram(grid, options);
    
    // Don't need the following if we're just parsing the file.
    RenderedImage image = new BitmapRenderer().renderToImage(diagram, options.renderingOptions);
    ImageIO.write(image, "png", os);
  #+END_SRC

  And thence: [[https://github.com/stathissideris/ditaa/blob/master/src/org/stathissideris/ascii2image/graphics/Diagram.java#L894][diagram.getShapeIterator]]; also [[https://github.com/stathissideris/ditaa/blob/master/src/org/stathissideris/ascii2image/graphics/DiagramShape.java#L500][shape.getEdges]]; also
  [[https://github.com/stathissideris/ditaa/blob/master/src/org/stathissideris/ascii2image/graphics/ShapeEdge.java#L149][edge.getOwner]], [[https://github.com/stathissideris/ditaa/blob/master/src/org/stathissideris/ascii2image/graphics/ShapeEdge.java#L121][shape.getEndPoint]], [[https://github.com/stathissideris/ditaa/blob/master/src/org/stathissideris/ascii2image/graphics/ShapeEdge.java#L121][edge.getStartPoint]].

  #+BEGIN_SRC clojure
    (import '(hewtoy UUIDToMD5Mapper
                     UUIDToMD5Mapper
                     MD5AndPayloadJoiner))
    
    (shell-task make-uuid-md5s "md5-uuid.sh")
    (map-reduce-task map-uuid-to-md5 UUIDToMD5Mapper)
    (map-reduce-task map-uuid-to-payload UUIDToMD5Mapper)
    (map-reduce-task join-md5-and-payload MD5AndPayloadJoiner)
    
    (dependencies
     (join-md5-and-payload
      ((map-uuid-to-md5
        (make-uuid-md5s))
       (map-uuid-to-payload
        (make-uuid-md5s)))))
    
    (dependencies join-md5-and-payload
                  (map-uuid-to-md5 make-uuid-md5s)
                  (map-uuid-to-payload make-uuid-md5s))
    
    (root join-md5-and-payload)
    
  #+END_SRC
* DONE Attach-geocode POC
  CLOSED: [2011-12-30 Fri 10:51]
  #+BEGIN_EXAMPLE
    geocode-data(i3): /apps/extract/poi/UnitedKingdomScarecrow/input/geocode_data
    md5-uuid-mapping(i2): /apps/extract/poi/UnitedKingdomScarecrow/output/split_test/014_compute_uuids
    deduped_entities(i1): /apps/extract/poi/UnitedKingdomScarecrow/output/split_test/020_combined_deduped_and_validation_data
  #+END_EXAMPLE

  I.e. [[http://d22.factual.com.:50075/browseDirectory.jsp?dir=%2Fapps%2Fextract%2Fpoi%2FUnitedKingdomScarecrow%2Finput%2Fgeocode_data&namenodeInfoPort=50070&delegation=null][geocode_data]], [[http://d22.factual.com.:50075/browseDirectory.jsp?dir=%2Fapps%2Fextract%2Fpoi%2FUnitedKingdomScarecrow%2Foutput%2Fsplit_test%2F014_compute_uuids&namenodeInfoPort=50070&delegation=null][compute_uuids]], [[http://d22.factual.com.:50075/browseDirectory.jsp?dir=%2Fapps%2Fextract%2Fpoi%2FUnitedKingdomScarecrow%2Foutput%2Fsplit_test%2F022_combined_deduped_and_validation_and_geocoding_data&namenodeInfoPort=50070&delegation=null][combined_deduped_and_validation_data]].

  [[https://github.com/Factual/back/blob/master/datastore-objects/src/main/thrift/factual_data_objects.thrift][Thrift-spec]]: payload and payloadRaw are JSON; [[https://github.com/Factual/back/blob/master/datastore-objects/src/main/java/com/factual/adaptors/Input.java][Wraps the input-data
  object]]: i.e. parses the JSON, provides a map.

  "Attach": append it to the array data-objects; eventually:
  summarization merges the array of data-objects. Rules: mode, mean;
  more complex rules, e.g. this came from that source and has a higher
  score.

  Tab-delimited data; sequence files: key-type, value-type.

  uuid -> data; md5 -> uuid; md5 -> geodata; [[http://d11.factual.com:50075/browseBlock.jsp?blockId=-901183859042176514&blockSize=30109191&genstamp=13911775&filename=%2Fapps%2Fextract%2Fpoi%2FUnitedKingdomScarecrow%2Finput%2Fgeocode_data%2Fgeocode_data_2010_07_24&datanodePort=50010&namenodeInfoPort=50070&delegation=null][geodata]]. Represent the
  data as JSON; propagate geodata back.

  Generalized attachment: join, attach, summarize.

  - Task 1 (non-Hadoop)
    - Copy data to HDFS: comma-delimited md5-i -> uuid-i.
    - Input :: local file
    - Output :: md5->uuid (comma delimited)
  - Task 2 (cascalog?)
    - Convert output of task 1 to tab-delimited md5 -> uuid-i.
    - Input :: md5->uuid (comma delimited)
    - Output :: md5->uuid (tab delimited)
  - Task 3 (straight-up Java?)
    - Join task 2 with md5-i -> data-i, such that uuid-i -> data-i
      (tab delimited).
    - Input :: md5->uuid (tab delimited)
    - Output :: uuid->data (tab delimited)

  Simulates: non-Hadoop, Hadoop-transformation,
  Hadoop-join. (Cf. [[http://hadoop.apache.org/common/docs/stable/mapred_tutorial.html][this]], by the way.)

  #+BEGIN_SRC sh :tangle make-md5-to-uuid.sh :shebang #!/usr/bin/env bash
    n=${@:-100}
    
    for ((i = 0; i < n; i++)); do
        echo $(echo -n $i | openssl md5 | cut -d ' ' -f 2),$i
    done > md5-to-uuid
    
  #+END_SRC

  #+BEGIN_SRC clojure :tangle md5-uuid.clj :shebang #!/usr/bin/env clj
    (use 'add-classpath.core)
    (add-classpath "lib/*")
    (add-classpath "lib/dev/*")
    (use 'debug.core)
    (use 'clojure.java.io)
    (use 'clojure.string)
    
    (doseq [line (line-seq (reader "md5-uuid.txt"))]
      (let [[md5 uuid] (split line #",")]
        (println (format "%s\t%s" md5 uuid))))
    
  #+END_SRC

  #+BEGIN_SRC clojure :tangle uuid-data.clj :shebang #!/usr/bin/env clj
    (use 'add-classpath.core)
    (add-classpath "lib/*")
    (add-classpath "lib/dev/*")
    (use 'debug.core)
    (use 'clojure.java.io)
    (use 'clojure.string)
    (use 'clojure.data.json)
    
    (doseq [line (line-seq (reader *in*))]
      (let [[md5 uuid] (split line #"\t")]
        (println (format "%s\t%s" uuid (json-str {:uuid uuid})))))
    
  #+END_SRC

  #+BEGIN_SRC java :tangle UUIDToMD5Mapper.java
    import java.io.IOException;
    
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.mapred.*;
    import org.apache.hadoop.mapred.lib.*;
    import org.apache.hadoop.conf.*;
    import org.apache.hadoop.io.*;
    import org.apache.hadoop.util.*;
    
    public class UUIDToMD5Mapper {
    
        public static class Map extends MapReduceBase
            implements Mapper<LongWritable, Text, Text, Text> {
            public void map(LongWritable key,
                            Text MD5ToUUID,
                            OutputCollector<Text, Text> output,
                            Reporter reporter)
                throws IOException {
                String[] MD5AndUUID = MD5ToUUID.toString().split(",");
                String MD5 = MD5AndUUID[0];
                String UUID = MD5AndUUID[1];
                output.collect(new Text(UUID),
                               new Text(String.format("md5: %s", MD5)));
            }
        }
    
        public static void main(String[] argv) throws IOException {
            JobConf conf = new JobConf(UUIDToMD5Mapper.class);
            conf.setJobName("map-uuid-to-md5");
            conf.setOutputKeyClass(Text.class);
            conf.setOutputValueClass(Text.class);
            conf.setMapperClass(Map.class);
            conf.setReducerClass(IdentityReducer.class);
            conf.setInputFormat(TextInputFormat.class);
            // conf.setOutputFormat(TextOutputFormat.class);
            conf.setOutputFormat(SequenceFileOutputFormat.class);
            FileInputFormat.setInputPaths(conf, new Path("md5-to-uuid"));
            FileOutputFormat.setOutputPath(conf, new Path("uuid-to-md5"));
            JobClient.runJob(conf);
        }
    }
    
  #+END_SRC

  #+BEGIN_SRC sh :tangle map-uuid-to-md5.sh :shebang #!/usr/bin/env bash
    org-tangle TODO.org && \
        rm -frv uuid-to-md5 && \
        mkdir -v UUIDToMD5Mapper-classes;
    
    javac -cp $(hadoop classpath) -d UUIDToMD5Mapper-classes UUIDToMD5Mapper.java && \
        jar -cvf UUIDToMD5Mapper.jar -C UUIDToMD5Mapper-classes . && \
        hadoop jar UUIDToMD5Mapper.jar UUIDToMD5Mapper && \
        hadoop fs -cat uuid-to-md5/*
    
  #+END_SRC

  #+BEGIN_SRC java :tangle UUIDToPayloadMapper.java
    import java.io.IOException;
    
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.mapred.*;
    import org.apache.hadoop.mapred.lib.*;
    import org.apache.hadoop.conf.*;
    import org.apache.hadoop.io.*;
    import org.apache.hadoop.util.*;
    
    public class UUIDToPayloadMapper {
    
        public static class Map extends MapReduceBase
            implements Mapper<LongWritable, Text, Text, Text> {
            public void map(LongWritable key,
                            Text MD5ToUUID,
                            OutputCollector<Text, Text> output,
                            Reporter reporter)
                throws IOException {
                String[] MD5AndUUID = MD5ToUUID.toString().split(",");
                String MD5 = MD5AndUUID[0];
                String UUID = MD5AndUUID[1];
                output.collect(new Text(UUID),
                               new Text(String.format("time: %s",
                                                      System.currentTimeMillis())));
            }
        }
    
        public static void main(String[] argv) throws IOException {
            JobConf conf = new JobConf(UUIDToPayloadMapper.class);
            conf.setJobName("map-uuid-to-payload");
            conf.setOutputKeyClass(Text.class);
            conf.setOutputValueClass(Text.class);
            conf.setMapperClass(Map.class);
            conf.setReducerClass(IdentityReducer.class);
            conf.setInputFormat(TextInputFormat.class);
            // conf.setOutputFormat(TextOutputFormat.class);
            conf.setOutputFormat(SequenceFileOutputFormat.class);
            FileInputFormat.setInputPaths(conf, new Path("md5-to-uuid"));
            FileOutputFormat.setOutputPath(conf, new Path("uuid-to-payload"));
            JobClient.runJob(conf);
        }
    }
    
  #+END_SRC

  #+BEGIN_SRC sh :tangle map-uuid-to-payload.sh :shebang #!/usr/bin/env bash
    org-tangle TODO.org && \
        rm -frv uuid-to-payload && \
        mkdir -v UUIDToPayloadMapper-classes;
    
    javac -cp $(hadoop classpath) -d UUIDToPayloadMapper-classes UUIDToPayloadMapper.java && \
        jar -cvf UUIDToPayloadMapper.jar -C UUIDToPayloadMapper-classes . && \
        hadoop jar UUIDToPayloadMapper.jar UUIDToPayloadMapper && \
        hadoop fs -cat uuid-to-payload/*
    
  #+END_SRC

  Swap it: "UUID\tMD5" after the first job; input to the second job:
  tab-delimited values and the simulated payload; when reducing during
  the second job, should see UUID -> (md5, payload)?

  Output of second job: combine the md5 and payload (i.e. insert md5
  into payload).

  Using sequence-files instead of text-files should give me key-value
  pairs (and obviate the need for destructuring the tab).

  #+BEGIN_SRC java :tangle MD5AndPayloadJoiner.java
    import java.io.IOException;
    import java.util.*;
    
    import org.apache.commons.logging.Log;
    import org.apache.commons.logging.LogFactory;
    
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.mapred.*;
    import org.apache.hadoop.mapred.lib.*;
    import org.apache.hadoop.conf.*;
    import org.apache.hadoop.io.*;
    import org.apache.hadoop.util.*;
    
    public class MD5AndPayloadJoiner {
        public static class Reduce extends MapReduceBase
            implements Reducer<Text, Text, Text, Text> {
            public void reduce(Text UUID,
                               Iterator<Text> values,
                               OutputCollector<Text, Text> output,
                               Reporter reporter)
                throws IOException {
                StringBuilder data = new StringBuilder();
                while (values.hasNext()) {
                    data.append(String.format("%s ", values.next()));
                }
                output.collect(UUID, new Text(data.toString()));
            }
        }
    
        public static void main(String[] argv) throws IOException {
            JobConf conf = new JobConf(MD5AndPayloadJoiner.class);
            conf.setJobName("map-uuid-to-payload");
            conf.setOutputKeyClass(Text.class);
            conf.setOutputValueClass(Text.class);
            conf.setMapperClass(IdentityMapper.class);
            conf.setReducerClass(Reduce.class);
            conf.setInputFormat(SequenceFileInputFormat.class);
            conf.setOutputFormat(TextOutputFormat.class);
            // conf.setOutputFormat(SequenceFileOutputFormat.class);
            FileInputFormat.addInputPath(conf, new Path("uuid-to-md5"));
            FileInputFormat.addInputPath(conf, new Path("uuid-to-payload"));
            FileOutputFormat.setOutputPath(conf, new Path("md5-and-payload"));
            JobClient.runJob(conf);
        }
    }
    
  #+END_SRC

  #+BEGIN_SRC sh :tangle join-md5-and-payload.sh :shebang #!/usr/bin/env bash
    org-tangle TODO.org && \
        rm -frv md5-and-payload && \
        mkdir -v MD5AndPayloadJoiner-classes;
    
    javac -cp $(hadoop classpath):classes -d MD5AndPayloadJoiner-classes MD5AndPayloadJoiner.java && \
        jar -cvf MD5AndPayloadJoiner.jar -C MD5AndPayloadJoiner-classes . && \
        hadoop jar MD5AndPayloadJoiner.jar MD5AndPayloadJoiner && \
        hadoop fs -cat md5-and-payload/*
    
  #+END_SRC

  - https://github.com/Factual/vineyard/blob/master/hadoop/src/main/java/vineyard/hadoop/demojob/WordCounter.java
  - https://github.com/Factual/vineyard/blob/master/hadoop/pom.xml
  - http://wiki.corp.factual.com/display/ENG/Internal+Maven+Proxy+Repository
  - https://github.com/Factual/vineyard/blob/master/hadoop/src/test/java/vineyard/hadoop/Producer.java
  - http://maven.corp.factual.com/nexus/index.html#nexus-search;quick~vineyard_hadoop

    #+BEGIN_SRC sh
      zip hewtoy-1.0.0-SNAPSHOT-standalone.jar -d META-INF/OSGI.SF
    #+END_SRC

    https://github.com/technomancy/leiningen/issues/31

    (defjob map-uuid-to-md5 hewtoy/UUIDToMd5Mapper)
* CANCELED Run the POC.
  CLOSED: [2011-12-30 Fri 10:51]
  #+BEGIN_SRC sh :tangle run.sh :shebang #!/usr/bin/env bash
    rm -frv /tmp/wirklich && \
        cd ~/prg/clj/napa && \
        lein clean && \
        lein jar && \
        java -cp napa-1.0.0-SNAPSHOT.jar:/tmp/clojure-hadoop-new/clojure-hadoop-1.3.1-SNAPSHOT-standalone.jar \
          clojure_hadoop.job \
          -job napa.core/job \
          -input md5-uuid.txt \
          -output /tmp/wirklich && \
        java -cp /tmp/clojure-hadoop-new/clojure-hadoop-1.3.1-SNAPSHOT-standalone.jar \
          org.apache.hadoop.fs.FsShell \
          -text /tmp/wirklich/part-r-00000
    
  #+END_SRC
* CANCELED Example with clojure-hadoop
  CLOSED: [2011-12-30 Fri 10:51]
  #+BEGIN_SRC clojure :tangle hadoop.clj :shebang #!/usr/bin/env clj
    (use 'add-classpath.core)
    (add-classpath "lib/*")
    
  #+END_SRC
* CANCELED Hadoop in beanshell?
  CLOSED: [2011-12-30 Fri 10:51]
  #+BEGIN_SRC java :tangle hadoop.bsh :shebang #!/usr/bin/env bsh
    addClassPath("lib/ant-1.6.5.jar");
    addClassPath("lib/clojure-1.3.0.jar");
    addClassPath("lib/clojure-contrib-1.2.0.jar");
    addClassPath("lib/clojure-hadoop-1.3.1-20110417.030036-1.jar");
    addClassPath("lib/commons-cli-1.2.jar");
    addClassPath("lib/commons-codec-1.3.jar");
    addClassPath("lib/commons-el-1.0.jar");
    addClassPath("lib/commons-httpclient-3.0.1.jar");
    addClassPath("lib/commons-logging-1.0.3.jar");
    addClassPath("lib/commons-net-1.4.1.jar");
    addClassPath("lib/core-3.1.1.jar");
    addClassPath("lib/hadoop-core-0.20.2.jar");
    addClassPath("lib/hsqldb-1.8.0.10.jar");
    addClassPath("lib/jasper-compiler-5.5.12.jar");
    addClassPath("lib/jasper-runtime-5.5.12.jar");
    addClassPath("lib/jets3t-0.7.1.jar");
    addClassPath("lib/jetty-6.1.14.jar");
    addClassPath("lib/jetty-util-6.1.14.jar");
    addClassPath("lib/jsp-2.1-6.1.14.jar");
    addClassPath("lib/jsp-api-2.1-6.1.14.jar");
    addClassPath("lib/junit-4.5.jar");
    addClassPath("lib/kfs-0.3.jar");
    addClassPath("lib/log4j-1.2.16.jar");
    addClassPath("lib/oro-2.0.8.jar");
    addClassPath("lib/servlet-api-2.5-6.1.14.jar");
    addClassPath("lib/xmlenc-0.52.jar");
    
    import java.util.*;
    
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.mapred.*;
    import org.apache.hadoop.conf.*;
    import org.apache.hadoop.io.*;
    import org.apache.hadoop.util.*;
    
    class Map extends MapReduceBase implements Mapper {
        one = new IntWritable(1);
        word = new Text();
    
        map(key, value, output, reporter) {
            line = value.toString();
            tokenizer = new StringTokenizer(line);
            while (tokenizer.hasMoreTokens()) {
                word.set(tokenizer.nextToken());
                output.collect(word, one);
            }
        }
    }
    
    class Reduce extends MapReduceBase implements Reducer {
        reduce(key, values, output, reporter) {
            int sum = 0;
            while (values.hasNext()) {
                sum += value.next().get();
            }
            output.collect(key, new IntWritable(sum));
        }
    }
    
    conf = new JobConf();
    conf.setJobName("wordcount");
    conf.setOutputKeyClass(Text.class);
    conf.setOutputValueClass(IntWritable.class);
    
    conf.setMapperClass(Map.class);
    conf.setCombinerClass(Reduce.class);
    conf.setReducerClass(Reduce.class);
    
    conf.setInputFormat(TextInputFormat.class);
    conf.setOutputFormat(TextOutputFormat.class);
    
    FileInputFormat.setInputPaths(conf, new Path("in"));
    FileInputFormat.setOutputPath(conf, new Path("out"));
    
    JobClient.runJob(conf);
    
  #+END_SRC
* CANCELED Analogy with cascalag-checkpoint
  CLOSED: [2011-12-30 Fri 10:51]
  From Aaron:

  #+BEGIN_QUOTE
  Props to Chun for pointing this out. Has some striking parallels to
  some of our requirements, so maybe a great source of inspiration for
  syntax. http://sritchie.github.com/2011/11/15/introducing-cascalogcontrib.html
                                                                                                                                                                                                                                                                                                                                                                                                                                         
  Notice for example there's an implicit naming convention for
  specifying sub tasks that run in parallel, vs. in series.
  #+END_QUOTE

  I do like the symbolic temporary directories; Vineyard's going to
  have to reap them appropriately, though.

  Rebind =read= in someone else's namespace?
* CANCELED Spec
  CLOSED: [2011-12-30 Fri 10:51]
  If we have:

  #+BEGIN_SRC clojure
    (deftask b
      :children (c d e)
      :dependencies (a))
  #+END_SRC

  I also want:

  #+BEGIN_SRC clojure
    (deftask a ...)
    (deftask b ...)
    (deftask c ...)
    ...
    
    (make-task-tree!
     (a
      (b
       (c d e))))
    
    (make-dependency-tree!
     (a
      (b)))
  #+END_SRC

  where tasks are created with the default settings, if they don't
  exist; possibly with a warning on stdout.
* CANCELED Proof-of-concept
  CLOSED: [2011-12-30 Fri 10:52]
  Chain two map-reduce tasks together. Vineyardize the tasks (without
  napa).

  Bogus wordcount example?

  Capitalize, count.

  Over hadoop.

  Non-hadoop precondition: moves local file with noisy words to HDFS;
  in hadoop: normalization (upper-case) and count.

  Output: word to count mapping:

  #+BEGIN_EXAMPLE
    ASS 1
    DONKEY 10
  #+END_EXAMPLE

  Validation: validating counters (name of counter, value),
  hdfs-file-exists?, hdfs-file-empty?

  #+BEGIN_SRC clojure
    (defn hadoop-counter [counter-name]
      ...)
    
    (defn call-with-hadoop-conditions [f]
      (f *hadoop-conditions*))
    
    (defn non-zero-hadoop-conditions? []
      (call-with-hadoop-conditions
        (fn [hadoop-conditions]
          (> (count hadoop-conditions) 0))))
    
    (if (non-zero-hadoop-conditions?)
      (throw ...Exception))
    
    (defn get-hadoop-job [vineyard-task]
      (...))
    
    (defn get-hadoop-counter [vineyard-task counter-name]
      (...))
    
    (defn get-hadoop-property [vineyard-ask property-name]
      (...))
    
    (> (get-hadoop-counter *vineyard-task* "foo") 0)
    
    (defn hdfs-file-exists? [vineyard-task path]
      ;; Check for the existence of _SUCCESS.
      (...))
    
    (hdfs-file-exists? *vineyard-task* "/path/to/dedupe")
    
    ;;; Inside pre-dedupe-analysis; path defaults to "/path/to/dedupe". In
    ;;; other words, "does the default input path of my parent exist?"
    (hdfs-file-parent-exists? *vineyard-task*)
    
  #+END_SRC

  [[http://wiki.corp.factual.com/display/INFRA/Vineyard+Java+Driver][Vineyard Java client]]. MapReduce jobs in Clojure? And pre-existing
  code in Java.

* CANCELED Extract POC
  CLOSED: [2011-12-30 Fri 10:52]
  [[https://github.com/Factual/hadoop-extraction-workflow/blob/master/src/java/workflows/extract/poi/UnitedStatesExtraction.java][US-extraction]]; enumerated subtasks:

  #+BEGIN_SRC java
    List<mapreduce.Task> tasks =
        Lists.newArrayList
        (
         writeHeaders,
         computeUniqueInputs,
         convertGeocodingResultsToTab,
         convertValidationJsonResultsToTab,
         writeUuidRetentionMappingToSequenceFile,
         extractEntities,
         computeSortedUniqueMd5s,
         analyzeExtractedEntities,
         postProcessExtractedData,
         preDedupeAnalysis,
         preDedupeAnalysisSummary,
         generateLikelyDupeMd5s,
         uniquifyLikelyDupeMd5s,
         computeDedupeUuids,
         assignUuids,
         groupDedupedEntities,
         dedupeQA,
         removeJunkInputsAndEntities,
         assignUuidsToValidationResults,
         combineDedupedAndValidationData,
         assignUuidsToGeocodingResults,
         combineDedupedAndGeocodingData,
         performFinalPostprocessing,
    
         computeUuidRetentionMapping,
         removeOverfoldingRetainedUuids,
         applyUuidRetentionMapping,
    
         exportData,
         exportDataQA,
    
         uuidRetentionTracker
         );
    
  #+END_SRC

  [[http://d22.factual.com.:50075/browseDirectory.jsp?dir=%2Fapps%2Fextract%2Fpoi%2FUnitedKingdomScarecrow%2Foutput%2Fleo_uuid_test&namenodeInfoPort=50070&delegation=null][Output]].

  #+BEGIN_SRC sh
    sudo hadoop jar hadoop-extraction-workflow-hadoop.jar \
        workflows.extract.poi.UnitedKingdomExtraction \
        hadoop_config_file=conf/mapreduce/MapReduceRunner/n_cluster.properties \
        extraction_config_class=extract.poi.UnitedKingdomScarecrow \
        project_name=UK_scarecrow_extraction_test \
        extraction_dataset_id=G4YzkQ \
        summary_view_id=cZqm0N
  #+END_SRC

  Modules:

  - Extract
  - Dedupe
  - Attach geo
  - Attach validation
  - UUID retention

  #+BEGIN_SRC lisp
    ;;; Grouping
    (hadoop-extraction-workflow
     (extract
      ^{predecessors: (extract)}
      (write-headers
       compute-unique-inputs
       convert-geocoding-results-to-tab
       convert-validation-json-results-to-tab
       write-uuid-retention-mapping-to-sequence-file
       extract-entities
       compute-sorted-unique-md5s
       analyze-extracted-entities
       post-process-extracted-data))
     (dedupe
      (pre-dedupe-analysis
       pre-dedupe-analysis-summary
       generate-likely-dupe-md5s
       uniquify-likely-dupe-md5s
       compute-dedupe-uuids
       assign-uuids
       group-deduped-entities
       dedupe-qa
       remove-junk-inputs-and-entities
       perform-final-postprocessing))
     (attach-geo
      (assign-uuids-to-geocoding-results
       combine-deduped-and-geocoding-data))
     (attach-validation
      (assign-uuids-to-validation-results
       combine-dedupe-and-validation-data))
     (uuid-retention
      (compute-uuid-retention-mapping
       remove-overfolding-retained-uuids
       apply-uuid-retention-mapping
       export-data
       export-data-qa
       uuid-retention-tracker)))
    
    ;;; Precedence
    (dedupe (extract))
    
    ;;; Gantt charts
    
    (a
     (b
      (c d)))
    
  #+END_SRC

* CANCELED =yaml= to vineyard
  CLOSED: [2011-12-30 Fri 10:52]
  We're going to have a =.onStart=, =.onFinish=; yaml leaves specify
  tasks. Have a predecessor thing:

  #+BEGIN_EXAMPLE
    iris
      chrome plugin
      nlp
    api
      places data
      sugar
    demo
      webui (depends 1, 4)
  #+END_EXAMPLE
* [[http://hadoop.apache.org/common/docs/stable/mapred_tutorial.html][MapReduce tutorial]]
  When loading data, load into =DistributedCache=; [[https://github.com/stuartsierra/clojure-hadoop][clojure-hadoop]].
* Notes
** Mon Dec 19 16:50:50 PST 2011   
   - at the end of task: check succeeded (Vineyard task); it fails;
     responsibility of the vineyard task to fail;
   - takes YAML: turns into command-line options
   - napa is the consumer that can run in daemon or cli mode (latter:
     takes yaml file, presents
   - name of the yaml file, config-argument
   - folders of yaml files
   - yaml files exist in scarecrow?
   - yaml files served up by screws?
   - composition of yaml-files?
   - "this step is actually this file"

** Tue Dec 27 09:56:33 PST 2011
   - command-line stuff: automatically parse the yaml: populate
     command line opts
   - workflow definition language
