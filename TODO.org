* TODO HEW
  Extract dedupe as library; generalize dedupe. Let's Vineyardize HEW
  without HEW knowing.

  #+BEGIN_SRC clojure :tangle hew-sans-vineyard.clj :shebang #!/usr/bin/env clj
    (use 'add-classpath.core)
    (add-classpath "lib/*")
    (add-classpath "lib/dev/*")
    
    (import '(mapreduce.tasks ConvertGeocodingResultsToTab)
            '(vineyard.hadoop MapReduceJobCreator
                              MapReduceTask)
            '(vineyard Task
                       TaskQueue))
    
    (def job
      (proxy [MapReduceJobCreator] []
        (createJob [configuration]
          (.createJob (ConvertGeocodingResultsToTab.)))))
    
    (.runAll (TaskQueue. "task07" 8081 "test")
             (doto (MapReduceTask.)
               (.setInitData "mr_job_class" (.getCanonicalName (class job)))))
    
  #+END_SRC
* DONE Job-spec
  CLOSED: [2012-01-17 Tue 15:17]
  Two functions: a =defjob= which takes a symbol, qualified class name
  and optional metadata (should be exported into a namespace for
  composability); =set-dependencies!= which takes a dependency graph
  of jobs.

  #+BEGIN_SRC clojure
    (load-file "othertoy.clj")
    
    (defn def-task [class init-data]
      (doto (new class)
        (set-init-data)))
    
    (defn def-hadoop-task []
      (def-task MapReduceTask {:blah "blah"}))
    
    (defn def-shell-task [command]
      )
    
    
    (deftask make-uuid-md5s hewtoy/UUIDMD5Maker)
    
    (deftask ^{:dependencies [make-uuid-md5s]
               :input-path "some-input-path"
               :task-type hadoop}
      uuid-to-payload-mapper hewtoy/UUIDToPayloadMapper)
    
    (deftask ^{:dependencies [make-uuid-md5s]}
      uuid-to-md5-mapper hewtoy/UUIDToMD5Mapper)
    
    (deftask md5-and-payload-joiner hewtoy/MD5AndPayloadJoiner)
    
    (deftask ls-tmp hewtoy/NewLS)
    
    (set-dependencies!
     (md5-and-payload-joiner
      ((uuid-to-md5-mapper
        make-uuid-md5s)
       (uuid-to-payload-mapper
        make-uuid-md5s
        randomtask))))
    
    (set-root! md5-and-payload-joiner)
    
  #+END_SRC

  Build out the root; send the root to vineyard. Have hadoop consumers
  already running.

  For the time being, comma delimited list: "input_paths"; also:
  java-interop;

  Draw dependency graphs: napa or vineyard?

  We need to distinguish between different classes of tasks: there's
  vanilla Vineyard tasks, but also shell-tasks, hadoop-tasks.

  General pattern: instantiate some nullary constructor; specify
  initData.

  Clojure driver based on the [[https://github.com/Factual/vineyard-java-driver][Java driver]].

  Two axes: dependency vs. predecessorship; expressed independently,
  too. See [[http://en.wikipedia.org/wiki/Tree_(graph_theory)#Definitions][this]], by the way:

  #+BEGIN_QUOTE
  An ordered tree or plane tree is a rooted tree for which an ordering
  is specified for the children of each vertex.
  #+END_QUOTE

  Also:

  #+BEGIN_QUOTE
  The term hedge sometimes refers to an ordered sequence of trees.
  #+END_QUOTE

  Are we dealing with a partially ordered tree? Where ordering
  expresses synchronicity (sequence).

  Generic =make-task= interface with a class and an init-data map;
  gets specialized into =make-hadoop-task= (or, alternatively,
  =task=), =make-shell-task= (=shell-task=), &c.

  =task=, =hadoop-task= (=map-reduce-task=, for that matter),
  =shell-task= are good if we're going declaritive; =dependency-graph=
  (or simply =dependencies=), furthermore; as well as
  =predecessor-graph= (or simply =predecessors=).[fn:: We'd only need
  to specify =-graph= is there was some meaningful distinction, such
  as e.g. =-tree=?] (How to visually express dependency and
  predecessorship in the same graph, by the way: differently colored
  links? Two graphs? Numbers?)

  Also, the Clojure metadata stuff is pretty ugly; different mechanism
  for specifying the init-data?

  See [[http://en.wikipedia.org/wiki/Dataflow_programming][dataflow-programming]]; [[http://stackoverflow.com/questions/4565158/using-clojure-dataflow-programming-idioms][in Clojure]]; [[http://richhickey.github.com/clojure-contrib/dataflow-api.html][API]]. [[http://upload.wikimedia.org/wikipedia/en/3/33/FBP_3_block_diagram.jpg][Block diagramm]].

  [[http://ditaa.sourceforge.net/][ditaa]] is what I'm talking about: parse it and input it. [[https://github.com/stathissideris/ditaa][Source]]. Take
  a look at [[https://github.com/stathissideris/ditaa/blob/master/src/org/stathissideris/ascii2image/core/CommandLineConverter.java#L222][this]]; basically:

  #+BEGIN_SRC java
    TextGrid grid = new TextGrid();
    grid.loadFrom(fromFilename, options.processingOptions);
    Diagram diagram = new Diagram(grid, options);
    
    // Don't need the following if we're just parsing the file.
    RenderedImage image = new BitmapRenderer().renderToImage(diagram, options.renderingOptions);
    ImageIO.write(image, "png", os);
  #+END_SRC

  And thence: [[https://github.com/stathissideris/ditaa/blob/master/src/org/stathissideris/ascii2image/graphics/Diagram.java#L894][diagram.getShapeIterator]]; also [[https://github.com/stathissideris/ditaa/blob/master/src/org/stathissideris/ascii2image/graphics/DiagramShape.java#L500][shape.getEdges]]; also
  [[https://github.com/stathissideris/ditaa/blob/master/src/org/stathissideris/ascii2image/graphics/ShapeEdge.java#L149][edge.getOwner]], [[https://github.com/stathissideris/ditaa/blob/master/src/org/stathissideris/ascii2image/graphics/ShapeEdge.java#L121][shape.getEndPoint]], [[https://github.com/stathissideris/ditaa/blob/master/src/org/stathissideris/ascii2image/graphics/ShapeEdge.java#L121][edge.getStartPoint]].

  #+BEGIN_SRC clojure :tangle task-spec.clj
    (load "make-uuid-md5s.clj")
    
    (import '(hewtoy UUIDToMD5Mapper
                     UUIDToMD5Mapper
                     MD5AndPayloadJoiner))
    
    ;; (shell-task make-uuid-md5s "md5-uuid.sh")
    
    (map-reduce-task map-uuid-to-md5
                     UUIDToMD5Mapper
                     :dependencies (make-uuid-md5s))
    
    (map-reduce-task map-uuid-to-payload
                     UUIDToMD5Mapper
                     :dependencies (make-uuid-md5s))
    
    (map-reduce-task join-md5-and-payload
                     MD5AndPayloadJoiner
                     :dependencies (map-uuid-to-md5
                                    map-uuid-to-payload))
    
    (dependencies
     (join-md5-and-payload
      (map-uuid-to-md5
       make-uuid-md5s)
      (map-uuid-to-payload
       make-uuid-md5s)))
    
  #+END_SRC

  Change TaskQueue port from 8080 to 8081; parameters to TaskQueue:
  optional parameter on the command line: API server, port, name of
  the resource.

  When run locally, next won't get kicked off (unless the consumer is
  running, in which case the consumer will pick it up). Quick
  iteration on one job: jump start that one, see it run, &c.

  #+BEGIN_SRC java
    Q.addTask(i0);
    
    // Run right here, right now; limitation: not going to go to
    // next. Advantage: don't have to create uberjar.
    //
    // Creates data in $PWD.
    Q.jumpStart(i0);    
  #+END_SRC

  #+BEGIN_SRC clojure
    ;;; Multimethod: second argument possibly a sequence.
    (depend-on [map-uuid-to-md5
                map-uuid-to-payload]
               make-uuid-md5s)
  #+END_SRC

  #+BEGIN_SRC clojure :tangle topological-sort.clj :shebang #!/usr/bin/env clj
    (use 'add-classpath.core)
    
    (add-classpath "lib/*")
    (add-classpath "lib/dev/*")
    
    (use 'debug.core)
    (use 'cadr.core)
    (use 'lambda.core)
    (use 'clojure.set)
    (import '(vineyard.hadoop MapReduceTask)
            '(vineyard.shell ShellTask)
            '(vineyard Task
                       TaskQueue)
            '(hewtoy MD5AndPayloadJoiner
                     UUIDToPayloadMapper
                     UUIDToMD5Mapper))
    
    (defn make-map-reduce-task [name class input output]
      (doto (MapReduceTask.)
        (.setInitData "input_path" input)
        (.setInitData "output_path" output)
        (.setInitData "mr_job_name" name)
        (.setInitData "mr_job_class" (.getCanonicalName class))))
    
    (defn make-shell-task [path]
      (doto (ShellTask.)
        (.setInitData ShellTask/SHELL_CMD_KEY path)))
    
    (defrecord node [task children parents])
    
    (defn make-node [task]
      (node. task (atom #{}) (atom #{})))
    
    (def table
      {'join-md5-and-payload (make-node
                              (make-map-reduce-task
                               "join-md5-and-payload"
                               MD5AndPayloadJoiner
                               "uuid-to-payload,uuid-to-md5"
                               "md5-and-payload"))
       'map-uuid-to-md5 (make-node
                         (make-map-reduce-task
                          "map-uuid-to-md5"
                          UUIDToMD5Mapper
                          "md5-to-uuid"
                          "uuid-to-md5"))
       'map-uuid-to-payload (make-node
                             (make-map-reduce-task
                              "map-uuid-to-payload"
                              UUIDToPayloadMapper
                              "md5-to-uuid"
                              "uuid-to-payload"))
       'make-uuid-md5s (make-node
                        (make-shell-task
                         "./make-md5-to-uuid.sh"))})
    
    ;;; Don't have a graph yet; this is just a tree. Need to do
    ;;; table-lookup for idempotency.
    (let [graph '(join-md5-and-payload
                  (map-uuid-to-md5
                   make-uuid-md5s)
                  (map-uuid-to-payload
                   make-uuid-md5s))]
      (letfn [(plumb [parents graph]
                ;; (debug graph)
                (if (list? graph)
                  (let [parent (get table (car graph))
                        children (cdr graph)]
                    (do
                      ;; (debug parent parents children)
                      (doseq [child children]
                        (plumb (cons parent parents) child))))
                  (do
                    ;; (debug 'leaf
                    ;;        parents
                    ;;        (get table graph))
                    (loop [child (get table graph)
                           parents parents]
                      (if (not (empty? parents))
                        (let [parent (car parents)]
                          (do
                            (swap! (:parents child)
                                   (fn [parents]
                                     (conj parents parent)))
                            (swap! (:children parent)
                                     (fn [children]
                                       (conj children child)))
                              (recur (car parents)
                                     (cdr parents)))))))))]
        (plumb nil graph)
        #_(doseq [[task node] table] (debug task node))
        (letfn [(sort [graph]
                  (let [sources
                        (filter (λ [node]
                                  (zero? (count (deref (:parents node)))))
                                (vals table))
                        sortita (atom nil)]
                    #_(debug (map :name sources))
                    (loop [parents (set sources)
                           sortita nil]
                      (if (empty? parents)
                        sortita
                        (let [parent (car parents)
                              children (deref (:children parent))]
                          (doseq [child children]
                            (swap! (:parents child)
                                   (λ [parents]
                                     (disj parents parent))))
                          (let [sources
                                (filter (λ [node]
                                          (zero? (count (deref (:parents node)))))
                                        children)]
                            (recur (union (set sources)
                                          (disj parents parent))
                                   (cons parent sortita))))))))]
          (let [tasks (map :task (sort graph))] 
            (loop [task (car tasks)
                   next-tasks (cdr tasks)]
              (if (not (empty? next-tasks))
                (let [next-task (car next-tasks)]
                  (.addNext task next-task)
                  (recur next-task (cdr next-tasks)))))
            (.runAll (TaskQueue. "task07" 8081 "topo-sort")
                     (first tasks))
            #_(.jumpStart (TaskQueue. "task07" 8081 "topo-sort")
                          (cadr tasks))
            (doseq [task tasks]
              (debug (.getId task)))))))
    
  #+END_SRC

  Need to come up with a [[http://en.wikipedia.org/wiki/Minimum_spanning_tree][minimum spanning tree]], and some kind of
  symbol \to task mapping? That way, when we begin at the leaves; or:
  should we construct the tree such that Vineyard begins at the
  leaves?

  Do we need a step in the process which creates anonymous
  intermediate nodes?

  In clojure, we can't easily modify a list; therefore, might need
  some kind of ad-hoc graph structure where we can remove nodes, &c.?

  #+BEGIN_SRC clojure :tangle records.clj :shebang #!/usr/bin/env clj
    (use 'clojure.test)
    (defrecord harro [yes])
    (def harro-0 (harro. 0))
    
    ;;; Local (non-mutative) association
    (is (:yes (assoc harro-0 :yes 1) 1))
    
    ;;; Original the same
    (is (:yes harro-0) 0)
    
    (defrecord omg [for-reals])
    (def wirklich (omg. (atom 1)))
    
    ;;; Pre-mutation
    (is (deref (:for-reals wirklich)) 1)
    
    (swap! (:for-reals wirklich)
           (fn [for-reals] (+ 1 for-reals)))
    
    ;;; Post-mutation
    (is (deref (:for-reals wirklich)) 2)
    
  #+END_SRC
* DONE Given then graph, build the job.
  CLOSED: [2012-01-13 Fri 14:04]
  #+BEGIN_SRC clojure :tangle build-job.clj :shebang #!/usr/bin/env clj
    (use 'add-classpath.core)
    
    (add-classpath "lib/*")
    (add-classpath "lib/dev/*")
    
    (use 'debug.core)
    (use 'lambda.core)
    (use 'cadr.core)
    (use 'clojure.test)
    
    (import '(vineyard.hadoop MapReduceTask)
            '(vineyard.shell ShellTask)
            '(vineyard Task
                       TaskQueue)
            '(hewtoy MD5AndPayloadJoiner
                     UUIDToPayloadMapper
                     UUIDToMD5Mapper))
    
    (defn make-map-reduce-task [name class input output]
      (doto (MapReduceTask.)
        (.setInitData "input_path" input)
        (.setInitData "output_path" output)
        (.setInitData "mr_job_name" name)
        (.setInitData "mr_job_class" (.getCanonicalName class))))
    
    (defn make-shell-task [path]
      (doto (ShellTask.)
        (.setInitData ShellTask/SHELL_CMD_KEY path)))
    
    (defλ tree->graph [tree]
      (let [child->parents (transient {})
            parent->children (transient {})]
        (letfn [(plumb! [parents tree]
                  (if (list? tree)
                    ;; This is what forbids the implicit primes: we
                    ;; require a parent.
                    (let [parent (car tree)
                          children (cdr tree)]
                      (doseq [child children]
                        (plumb! (cons parent parents) child)))
                    (loop [child tree
                           parents parents]
                      (if (empty? parents)
                        ;; Empty parent-set for roots.
                        (assoc! child->parents
                                child
                                (get child->parents child nil))
                        (let [parent (car parents)]
                          (assoc! child->parents
                                  child
                                  (cons parent
                                        (get child->parents child nil)))
                          (assoc! parent->children
                                  parent
                                  (cons child
                                        (get parent->children parent nil)))
                          ;; Also: empty set for leaves.
                          (assoc! parent->children
                                  child
                                  (get parent->children child nil))
                          (recur (car parents)
                                 (cdr parents)))))))]
          (plumb! nil tree)
          {:child->parents (persistent! child->parents)
           :parent->children (persistent! parent->children)})))
    
    #_(defλ run-tree [tree]
        (let [graph (tree->graph tree)
              tasks (map :task (topological-sort graph))]
          (.runAll (TaskQueue. *default-host* *default-port* *default-resource*)
                   (car tasks))))
    
    (import '(hewtoy UUIDToMD5Mapper
                     UUIDToMD5Mapper
                     MD5AndPayloadJoiner))
    
    ;;; These procedures are distinct from the DSL that invokes them.
    (def join-md5-and-payload
      (make-map-reduce-task
       "join-md5-and-payload"
       MD5AndPayloadJoiner
       "uuid-to-payload,uuid-to-md5"
       "md5-and-payload"))
    
    (def map-uuid-to-md5
      (make-map-reduce-task
       "map-uuid-to-md5"
       UUIDToMD5Mapper
       "md5-to-uuid"
       "uuid-to-md5"))
    
    (def map-uuid-to-payload
      (make-map-reduce-task
       "map-uuid-to-payload"
       UUIDToPayloadMapper
       "md5-to-uuid"
       "uuid-to-payload"))
    
    (def make-uuid-md5s
      (make-shell-task
       "./make-md5-to-uuid.sh"))
    
    (defλ task-table [tasks]
      (reduce (λ [table task] (assoc table task (eval task)))
              {}
              tasks))
    
    (defλ build-job [parent->children root tasks]
      (let [queue (atom (clojure.lang.PersistentQueue/EMPTY))
            visited (transient #{})]
        (swap! queue (λ [queue] (conj queue root)))
        (conj! visited root)
        (while (not (empty? (deref queue)))
          (let [parent (peek (deref queue))]
            (swap! queue (λ [queue] (pop queue)))
            (let [children (get parent->children parent)]
              (doseq [child children]
                (if (not (get visited child))
                  (do
                    (.addFirstly (get tasks parent)
                                 (get tasks child))
                    (conj! visited child)
                    (swap! queue (λ [queue] (conj queue child)))))))))
        (get tasks root)))
    
    (def ^:dynamic *default-host* "task07")
    (def ^:dynamic *default-port* 8081)
    (def ^:dynamic *default-resource* "test")
    
    (let [{:keys [child->parents parent->children]}
          (tree->graph
           '(join-md5-and-payload
             (map-uuid-to-md5
              make-uuid-md5s)
             (map-uuid-to-payload
              make-uuid-md5s)))
          tasks (task-table '(join-md5-and-payload
                              map-uuid-to-md5
                              map-uuid-to-payload
                              make-uuid-md5s))]
      (let [root (build-job parent->children 'join-md5-and-payload tasks)]
        (.runAll (TaskQueue. *default-host* *default-port* *default-resource*)
                 root)))
    
  #+END_SRC
* DONE We could actually do topo without repetition if we had ordered sets.
  CLOSED: [2012-01-13 Fri 14:04]
  Either order on a secondary key or use [[https://github.com/flatland/ordered][ordered sets]]; damn, Alan even
  handles the transient case. Let's see how it behaves.
* DONE =tree->graph=
  CLOSED: [2012-01-13 Fri 14:04]
  #+BEGIN_SRC clojure :tangle tree-to-graph.clj :shebang #!/usr/bin/env clj
    (use 'add-classpath.core)
    
    (add-classpath "lib/*")
    (add-classpath "lib/dev/*")
    
    (use 'debug.core)
    (use 'lambda.core)
    (use 'cadr.core)
    (use 'clojure.test)
    
    ;;; We can also implement a destructive topological sort with
    ;;; `transient', by the way. Or not: can't iterate over them, or even
    ;;; extract the keys.
    (defλ tree->graph [tree]
      (let [child->parents (transient {})
            parent->children (transient {})]
        (letfn [(plumb! [parents tree]
                  (if (list? tree)
                    ;; This is what forbids the implicit primes: we
                    ;; require a parent.
                    (let [parent (car tree)
                          children (cdr tree)]
                      (doseq [child children]
                        (plumb! (cons parent parents) child)))
                    (loop [child tree
                           parents parents]
                      (if (not (empty? parents))
                        ;; Empty parent-set for roots.
                        (assoc! child->parents
                                child
                                (get child->parents child nil))
                        (let [parent (car parents)]
                          (assoc! child->parents
                                  child
                                  (cons parent
                                        (get child->parents child nil)))
                          (assoc! parent->children
                                  parent
                                  (cons child
                                        (get parent->children parent nil)))
                          ;; Also: empty set for leaves.
                          (assoc! parent->children
                                  child
                                  (get parent->children child nil))
                          (recur (car parents)
                                 (cdr parents)))))))]
          (plumb! nil tree)
          {:child->parents (persistent! child->parents)
           :parent->children (persistent! parent->children)})))
    
    (defλ sinks [parent->children]
      (map car (filter (λ [[parent children]] (zero? (count children)))
                       parent->children)))
    
    ;;; These don't give me sinks, by the way: merely symmetrical links.
    (defλ topological-sort [child->parents parent->children]
      (let [sinks (sinks parent->children)
            visitata (transient #{})
            sortita (transient [])]
        (letfn [(visit [child]
                  (if (not (get visitata child))
                    (let [parents (get child->parents child)]
                      (conj! visitata child)
                      (doseq [parent parents]
                        (visit parent))
                      (conj! sortita child))))]
          (doseq [sink sinks] (visit sink)))
        (persistent! sortita)))
    
    (let [tree '(a (b d) (c d))
          {:keys [child->parents parent->children]} (tree->graph tree)]
      (is (= (topological-sort child->parents parent->children)
             '[a c a b d])))
    
  #+END_SRC
* DONE Implement task-spec.
  CLOSED: [2012-01-13 Fri 14:04]
  #+BEGIN_SRC clojure :tangle task-spec.clj :shebang #!/usr/bin/env clj
    (use 'add-classpath.core)
    
    (add-classpath "lib/*")
    (add-classpath "lib/dev/*")
    
    (use 'debug.core)
    (use 'cadr.core)
    (use 'lambda.core)
    (use 'clojure.set)
    
    (import '(vineyard.hadoop MapReduceTask)
            '(vineyard.shell ShellTask)
            '(vineyard Task
                       TaskQueue)
            '(hewtoy MD5AndPayloadJoiner
                     UUIDToPayloadMapper
                     UUIDToMD5Mapper))
    
    (defn make-map-reduce-task [name class input output]
      (doto (MapReduceTask.)
        (.setInitData "input_path" input)
        (.setInitData "output_path" output)
        (.setInitData "mr_job_name" name)
        (.setInitData "mr_job_class" (.getCanonicalName class))))
    
    (defn make-shell-task [path]
      (doto (ShellTask.)
        (.setInitData ShellTask/SHELL_CMD_KEY path)))
    
    (defrecord node [task children parents])
    
    (defn make-node [task]
      (node. task (atom #{}) (atom #{})))
    
    ;;; This should somehow return an adjacency list and root; can we
    ;;; `assoc!' a children and parent map along the way, finally do
    ;;; `make-node' on the maps and designating a root? We need a third
    ;;; table of name to nodes, probably. If we traffic in symbols, the
    ;;; name -> node association can change without creating orphans.
    ;;;
    ;;; We really need a payload table; with the automatic idempotency of
    ;;; symbols. (It only makes sense to be idempotent with respect to a
    ;;; binary operation, by the way. Not true:
    ;;; <http://en.wikipedia.org/wiki/Idempotence#Unary_operation>. Quality
    ;;; of symbols such that self-referential?)
    (defλ tree->graph
      ([tree]
         (tree->graph nil tree))
      ([parents tree]
         (if (list? tree)
           (let [parent (get table (car tree))
                 children (cdr tree)]
             (do
               (doseq [child children]
                 (plumb (cons parent parents) child))))
           (do
             (loop [child (get table tree)
                    parents parents]
               (if (not (empty? parents))
                 (let [parent (car parents)]
                   (do
                     (swap! (:parents child)
                            (fn [parents]
                              (conj parents parent)))
                     (swap! (:children parent)
                            (fn [children]
                              (conj children child)))
                     (recur (car parents)
                            (cdr parents))))))))))
    
    (defλ topological-sort [graph]
      (let [sources
            (filter (λ [node]
                      (zero? (count (deref (:parents node)))))
                    (vals table))
            sortita (atom nil)]
        (loop [parents (set sources)
               sortita nil]
          (if (empty? parents)
            sortita
            (let [parent (car parents)
                  children (deref (:children parent))]
              (doseq [child children]
                (swap! (:parents child)
                       (λ [parents]
                         (disj parents parent))))
              (let [sources
                    (filter (λ [node]
                              (zero? (count (deref (:parents node)))))
                            children)]
                (recur (union (set sources)
                              (disj parents parent))
                       (cons parent sortita))))))))
    
    (def ^:dynamic *default-host* "task07")
    (def ^:dynamic *default-port* "8081")
    (def ^:dynamic *default-resource* "test")
    
    (defλ run-tree [tree]
      (let [graph (tree->graph tree)
            tasks (map :task (topological-sort graph))]
        (.runAll (TaskQueue. *default-host* *default-port* *default-resource*)
                 (car tasks))))
    
    (import '(hewtoy UUIDToMD5Mapper
                     UUIDToMD5Mapper
                     MD5AndPayloadJoiner))
    
    ;;; These procedures are distinct from the DSL that invokes them.
    (def join-md5-and-payload
      (make-map-reduce-task
       "join-md5-and-payload"
       MD5AndPayloadJoiner
       "uuid-to-payload,uuid-to-md5"
       "md5-and-payload"))
    
    (def map-uuid-to-md5
      (make-map-reduce-task
       "map-uuid-to-md5"
       UUIDToMD5Mapper
       "md5-to-uuid"
       "uuid-to-md5"))
    
    (def map-uuid-to-payload
      (make-map-reduce-task
       "map-uuid-to-payload"
       UUIDToPayloadMapper
       "md5-to-uuid"
       "uuid-to-payload"))
    
    (def make-uuid-md5s
      (make-shell-task
       "./make-md5-to-uuid.sh"))
    
    (tree->graph
     [join-md5-and-payload
      [map-uuid-to-md5
       make-uuid-md5s]
      [map-uuid-to-payload
       make-uuid-md5s]])
    
  #+END_SRC

  Some node-aware post-processing that e.g. sets the input/output
  paths in hadoop nodes; a post-graph-walk (just on the adjancency
  table?).

  Whilst sleeping: =tree->graph= creates nodes, etc.; =walk-graph=
  let's you modify the nodes (possibly with a dispatch table of type
  \to modifier); =enqueue-graph=, =enqueue-graph-topologically=,
  &c. Need to specify root{,s}?

  Should the DSL use e.g. fully qualified classes such that we can
  worry about resolution later (and treat them, basically, as strings;
  which is what Vineyard does)?
* DONE Job-spec
  CLOSED: [2012-01-13 Fri 14:04]
* DONE Attach-geocode POC
  CLOSED: [2011-12-30 Fri 10:51]
  #+BEGIN_EXAMPLE
    geocode-data(i3): /apps/extract/poi/UnitedKingdomScarecrow/input/geocode_data
    md5-uuid-mapping(i2): /apps/extract/poi/UnitedKingdomScarecrow/output/split_test/014_compute_uuids
    deduped_entities(i1): /apps/extract/poi/UnitedKingdomScarecrow/output/split_test/020_combined_deduped_and_validation_data
  #+END_EXAMPLE

  I.e. [[http://d22.factual.com.:50075/browseDirectory.jsp?dir=%2Fapps%2Fextract%2Fpoi%2FUnitedKingdomScarecrow%2Finput%2Fgeocode_data&namenodeInfoPort=50070&delegation=null][geocode_data]], [[http://d22.factual.com.:50075/browseDirectory.jsp?dir=%2Fapps%2Fextract%2Fpoi%2FUnitedKingdomScarecrow%2Foutput%2Fsplit_test%2F014_compute_uuids&namenodeInfoPort=50070&delegation=null][compute_uuids]], [[http://d22.factual.com.:50075/browseDirectory.jsp?dir=%2Fapps%2Fextract%2Fpoi%2FUnitedKingdomScarecrow%2Foutput%2Fsplit_test%2F022_combined_deduped_and_validation_and_geocoding_data&namenodeInfoPort=50070&delegation=null][combined_deduped_and_validation_data]].

  [[https://github.com/Factual/back/blob/master/datastore-objects/src/main/thrift/factual_data_objects.thrift][Thrift-spec]]: payload and payloadRaw are JSON; [[https://github.com/Factual/back/blob/master/datastore-objects/src/main/java/com/factual/adaptors/Input.java][Wraps the input-data
  object]]: i.e. parses the JSON, provides a map.

  "Attach": append it to the array data-objects; eventually:
  summarization merges the array of data-objects. Rules: mode, mean;
  more complex rules, e.g. this came from that source and has a higher
  score.

  Tab-delimited data; sequence files: key-type, value-type.

  uuid -> data; md5 -> uuid; md5 -> geodata; [[http://d11.factual.com:50075/browseBlock.jsp?blockId=-901183859042176514&blockSize=30109191&genstamp=13911775&filename=%2Fapps%2Fextract%2Fpoi%2FUnitedKingdomScarecrow%2Finput%2Fgeocode_data%2Fgeocode_data_2010_07_24&datanodePort=50010&namenodeInfoPort=50070&delegation=null][geodata]]. Represent the
  data as JSON; propagate geodata back.

  Generalized attachment: join, attach, summarize.

  - Task 1 (non-Hadoop)
    - Copy data to HDFS: comma-delimited md5-i -> uuid-i.
    - Input :: local file
    - Output :: md5->uuid (comma delimited)
  - Task 2 (cascalog?)
    - Convert output of task 1 to tab-delimited md5 -> uuid-i.
    - Input :: md5->uuid (comma delimited)
    - Output :: md5->uuid (tab delimited)
  - Task 3 (straight-up Java?)
    - Join task 2 with md5-i -> data-i, such that uuid-i -> data-i
      (tab delimited).
    - Input :: md5->uuid (tab delimited)
    - Output :: uuid->data (tab delimited)

  Simulates: non-Hadoop, Hadoop-transformation,
  Hadoop-join. (Cf. [[http://hadoop.apache.org/common/docs/stable/mapred_tutorial.html][this]], by the way.)

  #+BEGIN_SRC sh :tangle make-md5-to-uuid.sh :shebang #!/usr/bin/env bash
    n=${@:-100}
    
    for ((i = 0; i < n; i++)); do
        echo $(echo -n $i | openssl md5 | cut -d ' ' -f 2),$i
    done > md5-to-uuid
    
  #+END_SRC

  #+BEGIN_SRC clojure :tangle md5-uuid.clj :shebang #!/usr/bin/env clj
    (use 'add-classpath.core)
    (add-classpath "lib/*")
    (add-classpath "lib/dev/*")
    (use 'debug.core)
    (use 'clojure.java.io)
    (use 'clojure.string)
    
    (doseq [line (line-seq (reader "md5-uuid.txt"))]
      (let [[md5 uuid] (split line #",")]
        (println (format "%s\t%s" md5 uuid))))
    
  #+END_SRC

  #+BEGIN_SRC clojure :tangle uuid-data.clj :shebang #!/usr/bin/env clj
    (use 'add-classpath.core)
    (add-classpath "lib/*")
    (add-classpath "lib/dev/*")
    (use 'debug.core)
    (use 'clojure.java.io)
    (use 'clojure.string)
    (use 'clojure.data.json)
    
    (doseq [line (line-seq (reader *in*))]
      (let [[md5 uuid] (split line #"\t")]
        (println (format "%s\t%s" uuid (json-str {:uuid uuid})))))
    
  #+END_SRC

  #+BEGIN_SRC java :tangle UUIDToMD5Mapper.java
    import java.io.IOException;
    
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.mapred.*;
    import org.apache.hadoop.mapred.lib.*;
    import org.apache.hadoop.conf.*;
    import org.apache.hadoop.io.*;
    import org.apache.hadoop.util.*;
    
    public class UUIDToMD5Mapper {
    
        public static class Map extends MapReduceBase
            implements Mapper<LongWritable, Text, Text, Text> {
            public void map(LongWritable key,
                            Text MD5ToUUID,
                            OutputCollector<Text, Text> output,
                            Reporter reporter)
                throws IOException {
                String[] MD5AndUUID = MD5ToUUID.toString().split(",");
                String MD5 = MD5AndUUID[0];
                String UUID = MD5AndUUID[1];
                output.collect(new Text(UUID),
                               new Text(String.format("md5: %s", MD5)));
            }
        }
    
        public static void main(String[] argv) throws IOException {
            JobConf conf = new JobConf(UUIDToMD5Mapper.class);
            conf.setJobName("map-uuid-to-md5");
            conf.setOutputKeyClass(Text.class);
            conf.setOutputValueClass(Text.class);
            conf.setMapperClass(Map.class);
            conf.setReducerClass(IdentityReducer.class);
            conf.setInputFormat(TextInputFormat.class);
            // conf.setOutputFormat(TextOutputFormat.class);
            conf.setOutputFormat(SequenceFileOutputFormat.class);
            FileInputFormat.setInputPaths(conf, new Path("md5-to-uuid"));
            FileOutputFormat.setOutputPath(conf, new Path("uuid-to-md5"));
            JobClient.runJob(conf);
        }
    }
    
  #+END_SRC

  #+BEGIN_SRC sh :tangle map-uuid-to-md5.sh :shebang #!/usr/bin/env bash
    org-tangle TODO.org && \
        rm -frv uuid-to-md5 && \
        mkdir -v UUIDToMD5Mapper-classes;
    
    javac -cp $(hadoop classpath) -d UUIDToMD5Mapper-classes UUIDToMD5Mapper.java && \
        jar -cvf UUIDToMD5Mapper.jar -C UUIDToMD5Mapper-classes . && \
        hadoop jar UUIDToMD5Mapper.jar UUIDToMD5Mapper && \
        hadoop fs -cat uuid-to-md5/*
    
  #+END_SRC

  #+BEGIN_SRC java :tangle UUIDToPayloadMapper.java
    import java.io.IOException;
    
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.mapred.*;
    import org.apache.hadoop.mapred.lib.*;
    import org.apache.hadoop.conf.*;
    import org.apache.hadoop.io.*;
    import org.apache.hadoop.util.*;
    
    public class UUIDToPayloadMapper {
    
        public static class Map extends MapReduceBase
            implements Mapper<LongWritable, Text, Text, Text> {
            public void map(LongWritable key,
                            Text MD5ToUUID,
                            OutputCollector<Text, Text> output,
                            Reporter reporter)
                throws IOException {
                String[] MD5AndUUID = MD5ToUUID.toString().split(",");
                String MD5 = MD5AndUUID[0];
                String UUID = MD5AndUUID[1];
                output.collect(new Text(UUID),
                               new Text(String.format("time: %s",
                                                      System.currentTimeMillis())));
            }
        }
    
        public static void main(String[] argv) throws IOException {
            JobConf conf = new JobConf(UUIDToPayloadMapper.class);
            conf.setJobName("map-uuid-to-payload");
            conf.setOutputKeyClass(Text.class);
            conf.setOutputValueClass(Text.class);
            conf.setMapperClass(Map.class);
            conf.setReducerClass(IdentityReducer.class);
            conf.setInputFormat(TextInputFormat.class);
            // conf.setOutputFormat(TextOutputFormat.class);
            conf.setOutputFormat(SequenceFileOutputFormat.class);
            FileInputFormat.setInputPaths(conf, new Path("md5-to-uuid"));
            FileOutputFormat.setOutputPath(conf, new Path("uuid-to-payload"));
            JobClient.runJob(conf);
        }
    }
    
  #+END_SRC

  #+BEGIN_SRC sh :tangle map-uuid-to-payload.sh :shebang #!/usr/bin/env bash
    org-tangle TODO.org && \
        rm -frv uuid-to-payload && \
        mkdir -v UUIDToPayloadMapper-classes;
    
    javac -cp $(hadoop classpath) -d UUIDToPayloadMapper-classes UUIDToPayloadMapper.java && \
        jar -cvf UUIDToPayloadMapper.jar -C UUIDToPayloadMapper-classes . && \
        hadoop jar UUIDToPayloadMapper.jar UUIDToPayloadMapper && \
        hadoop fs -cat uuid-to-payload/*
    
  #+END_SRC

  Swap it: "UUID\tMD5" after the first job; input to the second job:
  tab-delimited values and the simulated payload; when reducing during
  the second job, should see UUID -> (md5, payload)?

  Output of second job: combine the md5 and payload (i.e. insert md5
  into payload).

  Using sequence-files instead of text-files should give me key-value
  pairs (and obviate the need for destructuring the tab).

  #+BEGIN_SRC java :tangle MD5AndPayloadJoiner.java
    import java.io.IOException;
    import java.util.*;
    
    import org.apache.commons.logging.Log;
    import org.apache.commons.logging.LogFactory;
    
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.mapred.*;
    import org.apache.hadoop.mapred.lib.*;
    import org.apache.hadoop.conf.*;
    import org.apache.hadoop.io.*;
    import org.apache.hadoop.util.*;
    
    public class MD5AndPayloadJoiner {
        public static class Reduce extends MapReduceBase
            implements Reducer<Text, Text, Text, Text> {
            public void reduce(Text UUID,
                               Iterator<Text> values,
                               OutputCollector<Text, Text> output,
                               Reporter reporter)
                throws IOException {
                StringBuilder data = new StringBuilder();
                while (values.hasNext()) {
                    data.append(String.format("%s ", values.next()));
                }
                output.collect(UUID, new Text(data.toString()));
            }
        }
    
        public static void main(String[] argv) throws IOException {
            JobConf conf = new JobConf(MD5AndPayloadJoiner.class);
            conf.setJobName("map-uuid-to-payload");
            conf.setOutputKeyClass(Text.class);
            conf.setOutputValueClass(Text.class);
            conf.setMapperClass(IdentityMapper.class);
            conf.setReducerClass(Reduce.class);
            conf.setInputFormat(SequenceFileInputFormat.class);
            conf.setOutputFormat(TextOutputFormat.class);
            // conf.setOutputFormat(SequenceFileOutputFormat.class);
            FileInputFormat.addInputPath(conf, new Path("uuid-to-md5"));
            FileInputFormat.addInputPath(conf, new Path("uuid-to-payload"));
            FileOutputFormat.setOutputPath(conf, new Path("md5-and-payload"));
            JobClient.runJob(conf);
        }
    }
    
  #+END_SRC

  #+BEGIN_SRC sh :tangle join-md5-and-payload.sh :shebang #!/usr/bin/env bash
    org-tangle TODO.org && \
        rm -frv md5-and-payload && \
        mkdir -v MD5AndPayloadJoiner-classes;
    
    javac -cp $(hadoop classpath):classes -d MD5AndPayloadJoiner-classes MD5AndPayloadJoiner.java && \
        jar -cvf MD5AndPayloadJoiner.jar -C MD5AndPayloadJoiner-classes . && \
        hadoop jar MD5AndPayloadJoiner.jar MD5AndPayloadJoiner && \
        hadoop fs -cat md5-and-payload/*
    
  #+END_SRC

  - https://github.com/Factual/vineyard/blob/master/hadoop/src/main/java/vineyard/hadoop/demojob/WordCounter.java
  - https://github.com/Factual/vineyard/blob/master/hadoop/pom.xml
  - http://wiki.corp.factual.com/display/ENG/Internal+Maven+Proxy+Repository
  - https://github.com/Factual/vineyard/blob/master/hadoop/src/test/java/vineyard/hadoop/Producer.java
  - http://maven.corp.factual.com/nexus/index.html#nexus-search;quick~vineyard_hadoop

    #+BEGIN_SRC sh
      zip hewtoy-1.0.0-SNAPSHOT-standalone.jar -d META-INF/OSGI.SF
    #+END_SRC

    https://github.com/technomancy/leiningen/issues/31

    (defjob map-uuid-to-md5 hewtoy/UUIDToMd5Mapper)
* CANCELED Run the POC.
  CLOSED: [2011-12-30 Fri 10:51]
  #+BEGIN_SRC sh :tangle run.sh :shebang #!/usr/bin/env bash
    rm -frv /tmp/wirklich && \
        cd ~/prg/clj/napa && \
        lein clean && \
        lein jar && \
        java -cp napa-1.0.0-SNAPSHOT.jar:/tmp/clojure-hadoop-new/clojure-hadoop-1.3.1-SNAPSHOT-standalone.jar \
          clojure_hadoop.job \
          -job napa.core/job \
          -input md5-uuid.txt \
          -output /tmp/wirklich && \
        java -cp /tmp/clojure-hadoop-new/clojure-hadoop-1.3.1-SNAPSHOT-standalone.jar \
          org.apache.hadoop.fs.FsShell \
          -text /tmp/wirklich/part-r-00000
    
  #+END_SRC
* CANCELED Example with clojure-hadoop
  CLOSED: [2011-12-30 Fri 10:51]
  #+BEGIN_SRC clojure :tangle hadoop.clj :shebang #!/usr/bin/env clj
    (use 'add-classpath.core)
    (add-classpath "lib/*")
    
  #+END_SRC
* CANCELED Hadoop in beanshell?
  CLOSED: [2011-12-30 Fri 10:51]
  #+BEGIN_SRC java :tangle hadoop.bsh :shebang #!/usr/bin/env bsh
    addClassPath("lib/ant-1.6.5.jar");
    addClassPath("lib/clojure-1.3.0.jar");
    addClassPath("lib/clojure-contrib-1.2.0.jar");
    addClassPath("lib/clojure-hadoop-1.3.1-20110417.030036-1.jar");
    addClassPath("lib/commons-cli-1.2.jar");
    addClassPath("lib/commons-codec-1.3.jar");
    addClassPath("lib/commons-el-1.0.jar");
    addClassPath("lib/commons-httpclient-3.0.1.jar");
    addClassPath("lib/commons-logging-1.0.3.jar");
    addClassPath("lib/commons-net-1.4.1.jar");
    addClassPath("lib/core-3.1.1.jar");
    addClassPath("lib/hadoop-core-0.20.2.jar");
    addClassPath("lib/hsqldb-1.8.0.10.jar");
    addClassPath("lib/jasper-compiler-5.5.12.jar");
    addClassPath("lib/jasper-runtime-5.5.12.jar");
    addClassPath("lib/jets3t-0.7.1.jar");
    addClassPath("lib/jetty-6.1.14.jar");
    addClassPath("lib/jetty-util-6.1.14.jar");
    addClassPath("lib/jsp-2.1-6.1.14.jar");
    addClassPath("lib/jsp-api-2.1-6.1.14.jar");
    addClassPath("lib/junit-4.5.jar");
    addClassPath("lib/kfs-0.3.jar");
    addClassPath("lib/log4j-1.2.16.jar");
    addClassPath("lib/oro-2.0.8.jar");
    addClassPath("lib/servlet-api-2.5-6.1.14.jar");
    addClassPath("lib/xmlenc-0.52.jar");
    
    import java.util.*;
    
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.mapred.*;
    import org.apache.hadoop.conf.*;
    import org.apache.hadoop.io.*;
    import org.apache.hadoop.util.*;
    
    class Map extends MapReduceBase implements Mapper {
        one = new IntWritable(1);
        word = new Text();
    
        map(key, value, output, reporter) {
            line = value.toString();
            tokenizer = new StringTokenizer(line);
            while (tokenizer.hasMoreTokens()) {
                word.set(tokenizer.nextToken());
                output.collect(word, one);
            }
        }
    }
    
    class Reduce extends MapReduceBase implements Reducer {
        reduce(key, values, output, reporter) {
            int sum = 0;
            while (values.hasNext()) {
                sum += value.next().get();
            }
            output.collect(key, new IntWritable(sum));
        }
    }
    
    conf = new JobConf();
    conf.setJobName("wordcount");
    conf.setOutputKeyClass(Text.class);
    conf.setOutputValueClass(IntWritable.class);
    
    conf.setMapperClass(Map.class);
    conf.setCombinerClass(Reduce.class);
    conf.setReducerClass(Reduce.class);
    
    conf.setInputFormat(TextInputFormat.class);
    conf.setOutputFormat(TextOutputFormat.class);
    
    FileInputFormat.setInputPaths(conf, new Path("in"));
    FileInputFormat.setOutputPath(conf, new Path("out"));
    
    JobClient.runJob(conf);
    
  #+END_SRC
* CANCELED Analogy with cascalag-checkpoint
  CLOSED: [2011-12-30 Fri 10:51]
  From Aaron:

  #+BEGIN_QUOTE
  Props to Chun for pointing this out. Has some striking parallels to
  some of our requirements, so maybe a great source of inspiration for
  syntax. http://sritchie.github.com/2011/11/15/introducing-cascalogcontrib.html
                                                                                                                                                                                                                                                                                                                                                                                                                                         
  Notice for example there's an implicit naming convention for
  specifying sub tasks that run in parallel, vs. in series.
  #+END_QUOTE

  I do like the symbolic temporary directories; Vineyard's going to
  have to reap them appropriately, though.

  Rebind =read= in someone else's namespace?
* CANCELED Spec
  CLOSED: [2011-12-30 Fri 10:51]
  If we have:

  #+BEGIN_SRC clojure
    (deftask b
      :children (c d e)
      :dependencies (a))
  #+END_SRC

  I also want:

  #+BEGIN_SRC clojure
    (deftask a ...)
    (deftask b ...)
    (deftask c ...)
    ...
    
    (make-task-tree!
     (a
      (b
       (c d e))))
    
    (make-dependency-tree!
     (a
      (b)))
  #+END_SRC

  where tasks are created with the default settings, if they don't
  exist; possibly with a warning on stdout.
* CANCELED Proof-of-concept
  CLOSED: [2011-12-30 Fri 10:52]
  Chain two map-reduce tasks together. Vineyardize the tasks (without
  napa).

  Bogus wordcount example?

  Capitalize, count.

  Over hadoop.

  Non-hadoop precondition: moves local file with noisy words to HDFS;
  in hadoop: normalization (upper-case) and count.

  Output: word to count mapping:

  #+BEGIN_EXAMPLE
    ASS 1
    DONKEY 10
  #+END_EXAMPLE

  Validation: validating counters (name of counter, value),
  hdfs-file-exists?, hdfs-file-empty?

  #+BEGIN_SRC clojure
    (defn hadoop-counter [counter-name]
      ...)
    
    (defn call-with-hadoop-conditions [f]
      (f *hadoop-conditions*))
    
    (defn non-zero-hadoop-conditions? []
      (call-with-hadoop-conditions
        (fn [hadoop-conditions]
          (> (count hadoop-conditions) 0))))
    
    (if (non-zero-hadoop-conditions?)
      (throw ...Exception))
    
    (defn get-hadoop-job [vineyard-task]
      (...))
    
    (defn get-hadoop-counter [vineyard-task counter-name]
      (...))
    
    (defn get-hadoop-property [vineyard-ask property-name]
      (...))
    
    (> (get-hadoop-counter *vineyard-task* "foo") 0)
    
    (defn hdfs-file-exists? [vineyard-task path]
      ;; Check for the existence of _SUCCESS.
      (...))
    
    (hdfs-file-exists? *vineyard-task* "/path/to/dedupe")
    
    ;;; Inside pre-dedupe-analysis; path defaults to "/path/to/dedupe". In
    ;;; other words, "does the default input path of my parent exist?"
    (hdfs-file-parent-exists? *vineyard-task*)
    
  #+END_SRC

  [[http://wiki.corp.factual.com/display/INFRA/Vineyard+Java+Driver][Vineyard Java client]]. MapReduce jobs in Clojure? And pre-existing
  code in Java.

* CANCELED Extract POC
  CLOSED: [2011-12-30 Fri 10:52]
  [[https://github.com/Factual/hadoop-extraction-workflow/blob/master/src/java/workflows/extract/poi/UnitedStatesExtraction.java][US-extraction]]; enumerated subtasks:

  #+BEGIN_SRC java
    List<mapreduce.Task> tasks =
        Lists.newArrayList
        (
         writeHeaders,
         computeUniqueInputs,
         convertGeocodingResultsToTab,
         convertValidationJsonResultsToTab,
         writeUuidRetentionMappingToSequenceFile,
         extractEntities,
         computeSortedUniqueMd5s,
         analyzeExtractedEntities,
         postProcessExtractedData,
         preDedupeAnalysis,
         preDedupeAnalysisSummary,
         generateLikelyDupeMd5s,
         uniquifyLikelyDupeMd5s,
         computeDedupeUuids,
         assignUuids,
         groupDedupedEntities,
         dedupeQA,
         removeJunkInputsAndEntities,
         assignUuidsToValidationResults,
         combineDedupedAndValidationData,
         assignUuidsToGeocodingResults,
         combineDedupedAndGeocodingData,
         performFinalPostprocessing,
    
         computeUuidRetentionMapping,
         removeOverfoldingRetainedUuids,
         applyUuidRetentionMapping,
    
         exportData,
         exportDataQA,
    
         uuidRetentionTracker
         );
    
  #+END_SRC

  [[http://d22.factual.com.:50075/browseDirectory.jsp?dir=%2Fapps%2Fextract%2Fpoi%2FUnitedKingdomScarecrow%2Foutput%2Fleo_uuid_test&namenodeInfoPort=50070&delegation=null][Output]].

  #+BEGIN_SRC sh
    sudo hadoop jar hadoop-extraction-workflow-hadoop.jar \
        workflows.extract.poi.UnitedKingdomExtraction \
        hadoop_config_file=conf/mapreduce/MapReduceRunner/n_cluster.properties \
        extraction_config_class=extract.poi.UnitedKingdomScarecrow \
        project_name=UK_scarecrow_extraction_test \
        extraction_dataset_id=G4YzkQ \
        summary_view_id=cZqm0N
  #+END_SRC

  Modules:

  - Extract
  - Dedupe
  - Attach geo
  - Attach validation
  - UUID retention

  #+BEGIN_SRC lisp
    ;;; Grouping
    (hadoop-extraction-workflow
     (extract
      ^{predecessors: (extract)}
      (write-headers
       compute-unique-inputs
       convert-geocoding-results-to-tab
       convert-validation-json-results-to-tab
       write-uuid-retention-mapping-to-sequence-file
       extract-entities
       compute-sorted-unique-md5s
       analyze-extracted-entities
       post-process-extracted-data))
     (dedupe
      (pre-dedupe-analysis
       pre-dedupe-analysis-summary
       generate-likely-dupe-md5s
       uniquify-likely-dupe-md5s
       compute-dedupe-uuids
       assign-uuids
       group-deduped-entities
       dedupe-qa
       remove-junk-inputs-and-entities
       perform-final-postprocessing))
     (attach-geo
      (assign-uuids-to-geocoding-results
       combine-deduped-and-geocoding-data))
     (attach-validation
      (assign-uuids-to-validation-results
       combine-dedupe-and-validation-data))
     (uuid-retention
      (compute-uuid-retention-mapping
       remove-overfolding-retained-uuids
       apply-uuid-retention-mapping
       export-data
       export-data-qa
       uuid-retention-tracker)))
    
    ;;; Precedence
    (dedupe (extract))
    
    ;;; Gantt charts
    
    (a
     (b
      (c d)))
    
  #+END_SRC

* CANCELED =yaml= to vineyard
  CLOSED: [2011-12-30 Fri 10:52]
  We're going to have a =.onStart=, =.onFinish=; yaml leaves specify
  tasks. Have a predecessor thing:

  #+BEGIN_EXAMPLE
    iris
      chrome plugin
      nlp
    api
      places data
      sugar
    demo
      webui (depends 1, 4)
  #+END_EXAMPLE
* [[http://hadoop.apache.org/common/docs/stable/mapred_tutorial.html][MapReduce tutorial]]
  When loading data, load into =DistributedCache=; [[https://github.com/stuartsierra/clojure-hadoop][clojure-hadoop]].
* Notes
** Mon Dec 19 16:50:50 PST 2011   
   - at the end of task: check succeeded (Vineyard task); it fails;
     responsibility of the vineyard task to fail;
   - takes YAML: turns into command-line options
   - napa is the consumer that can run in daemon or cli mode (latter:
     takes yaml file, presents
   - name of the yaml file, config-argument
   - folders of yaml files
   - yaml files exist in scarecrow?
   - yaml files served up by screws?
   - composition of yaml-files?
   - "this step is actually this file"

** Tue Dec 27 09:56:33 PST 2011
   - command-line stuff: automatically parse the yaml: populate
     command line opts
   - workflow definition language
