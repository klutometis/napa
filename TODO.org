* TODO Proof-of-concept
  Chain two map-reduce tasks together. Vineyardize the tasks (without
  napa).

  Bogus wordcount example?

  Capitalize, count.

  Over hadoop.

  Non-hadoop precondition: moves local file with noisy words to HDFS;
  in hadoop: normalization (upper-case) and count.

  Output: word to count mapping:

  #+BEGIN_EXAMPLE
    ASS 1
    DONKEY 10
  #+END_EXAMPLE

  Validation: validating counters (name of counter, value),
  hdfs-file-exists?, hdfs-file-empty?

  #+BEGIN_SRC clojure
    (defn hadoop-counter [counter-name]
      ...)
    
    (defn call-with-hadoop-conditions [f]
      (f *hadoop-conditions*))
    
    (defn non-zero-hadoop-conditions? []
      (call-with-hadoop-conditions
        (fn [hadoop-conditions]
          (> (count hadoop-conditions) 0))))
    
    (if (non-zero-hadoop-conditions?)
      (throw ...Exception))
    
    (defn get-hadoop-job [vineyard-task]
      (...))
    
    (defn get-hadoop-counter [vineyard-task counter-name]
      (...))
    
    (defn get-hadoop-property [vineyard-ask property-name]
      (...))
    
    (> (get-hadoop-counter *vineyard-task* "foo") 0)
    
    (defn hdfs-file-exists? [vineyard-task path]
      ;; Check for the existence of _SUCCESS.
      (...))
    
    (hdfs-file-exists? *vineyard-task* "/path/to/dedupe")
    
    ;;; Inside pre-dedupe-analysis; path defaults to "/path/to/dedupe". In
    ;;; other words, "does the default input path of my parent exist?"
    (hdfs-file-parent-exists? *vineyard-task*)
    
  #+END_SRC

  [[http://wiki.corp.factual.com/display/INFRA/Vineyard+Java+Driver][Vineyard Java client]]. MapReduce jobs in Clojure? And pre-existing
  code in Java.

* TODO Extract POC
  [[https://github.com/Factual/hadoop-extraction-workflow/blob/master/src/java/workflows/extract/poi/UnitedStatesExtraction.java][US-extraction]]; enumerated subtasks:

  #+BEGIN_SRC java
    List<mapreduce.Task> tasks =
        Lists.newArrayList
        (
         writeHeaders,
         computeUniqueInputs,
         convertGeocodingResultsToTab,
         convertValidationJsonResultsToTab,
         writeUuidRetentionMappingToSequenceFile,
         extractEntities,
         computeSortedUniqueMd5s,
         analyzeExtractedEntities,
         postProcessExtractedData,
         preDedupeAnalysis,
         preDedupeAnalysisSummary,
         generateLikelyDupeMd5s,
         uniquifyLikelyDupeMd5s,
         computeDedupeUuids,
         assignUuids,
         groupDedupedEntities,
         dedupeQA,
         removeJunkInputsAndEntities,
         assignUuidsToValidationResults,
         combineDedupedAndValidationData,
         assignUuidsToGeocodingResults,
         combineDedupedAndGeocodingData,
         performFinalPostprocessing,
    
         computeUuidRetentionMapping,
         removeOverfoldingRetainedUuids,
         applyUuidRetentionMapping,
    
         exportData,
         exportDataQA,
    
         uuidRetentionTracker
         );
    
  #+END_SRC

  [[http://d22.factual.com.:50075/browseDirectory.jsp?dir=%2Fapps%2Fextract%2Fpoi%2FUnitedKingdomScarecrow%2Foutput%2Fleo_uuid_test&namenodeInfoPort=50070&delegation=null][Output]].

  #+BEGIN_SRC sh
    sudo hadoop jar hadoop-extraction-workflow-hadoop.jar \
        workflows.extract.poi.UnitedKingdomExtraction \
        hadoop_config_file=conf/mapreduce/MapReduceRunner/n_cluster.properties \
        extraction_config_class=extract.poi.UnitedKingdomScarecrow \
        project_name=UK_scarecrow_extraction_test \
        extraction_dataset_id=G4YzkQ \
        summary_view_id=cZqm0N
  #+END_SRC

  Modules:

  - Extract
  - Dedupe
  - Attach geo
  - Attach validation
  - UUID retention

  #+BEGIN_SRC lisp
    ;;; Grouping
    (hadoop-extraction-workflow
     (extract
      ^{predecessors: (extract)}
      (write-headers
       compute-unique-inputs
       convert-geocoding-results-to-tab
       convert-validation-json-results-to-tab
       write-uuid-retention-mapping-to-sequence-file
       extract-entities
       compute-sorted-unique-md5s
       analyze-extracted-entities
       post-process-extracted-data))
     (dedupe
      (pre-dedupe-analysis
       pre-dedupe-analysis-summary
       generate-likely-dupe-md5s
       uniquify-likely-dupe-md5s
       compute-dedupe-uuids
       assign-uuids
       group-deduped-entities
       dedupe-qa
       remove-junk-inputs-and-entities
       perform-final-postprocessing))
     (attach-geo
      (assign-uuids-to-geocoding-results
       combine-deduped-and-geocoding-data))
     (attach-validation
      (assign-uuids-to-validation-results
       combine-dedupe-and-validation-data))
     (uuid-retention
      (compute-uuid-retention-mapping
       remove-overfolding-retained-uuids
       apply-uuid-retention-mapping
       export-data
       export-data-qa
       uuid-retention-tracker)))
    
    ;;; Precedence
    (dedupe (extract))
    
    ;;; Gantt charts
    
    (a
     (b
      (c d)))
    
  #+END_SRC

* TODO =yaml= to vineyard
  We're going to have a =.onStart=, =.onFinish=; yaml leaves specify
  tasks. Have a predecessor thing:

  #+BEGIN_EXAMPLE
    iris
      chrome plugin
      nlp
    api
      places data
      sugar
    demo
      webui (depends 1, 4)
  #+END_EXAMPLE
* Notes
** Mon Dec 19 16:50:50 PST 2011   
   - at the end of task: check succeeded (Vineyard task); it fails;
     responsibility of the vineyard task to fail;
   - takes YAML: turns into command-line options
   - napa is the consumer that can run in daemon or cli mode (latter:
     takes yaml file, presents
   - name of the yaml file, config-argument
   - folders of yaml files
   - yaml files exist in scarecrow?
   - yaml files served up by screws?
   - composition of yaml-files?
   - "this step is actually this file"

