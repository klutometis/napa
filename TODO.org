* TODO Run the POC.
  #+BEGIN_SRC sh :tangle run.sh :shebang #!/usr/bin/env bash
    rm -frv /tmp/wirklich && \
        cd ~/prg/clj/napa && \
        lein clean && \
        lein jar && \
        java -cp napa-1.0.0-SNAPSHOT.jar:/tmp/clojure-hadoop-new/clojure-hadoop-1.3.1-SNAPSHOT-standalone.jar \
          clojure_hadoop.job \
          -job napa.core/job \
          -input md5-uuid.txt \
          -output /tmp/wirklich && \
        java -cp /tmp/clojure-hadoop-new/clojure-hadoop-1.3.1-SNAPSHOT-standalone.jar \
          org.apache.hadoop.fs.FsShell \
          -text /tmp/wirklich/part-r-00000
    
  #+END_SRC
* TODO Example with clojure-hadoop
  #+BEGIN_SRC clojure :tangle hadoop.clj :shebang #!/usr/bin/env clj
    (use 'add-classpath.core)
    (add-classpath "lib/*")
    
  #+END_SRC
* TODO Hadoop in beanshell?
  #+BEGIN_SRC java :tangle hadoop.bsh :shebang #!/usr/bin/env bsh
    addClassPath("lib/ant-1.6.5.jar");
    addClassPath("lib/clojure-1.3.0.jar");
    addClassPath("lib/clojure-contrib-1.2.0.jar");
    addClassPath("lib/clojure-hadoop-1.3.1-20110417.030036-1.jar");
    addClassPath("lib/commons-cli-1.2.jar");
    addClassPath("lib/commons-codec-1.3.jar");
    addClassPath("lib/commons-el-1.0.jar");
    addClassPath("lib/commons-httpclient-3.0.1.jar");
    addClassPath("lib/commons-logging-1.0.3.jar");
    addClassPath("lib/commons-net-1.4.1.jar");
    addClassPath("lib/core-3.1.1.jar");
    addClassPath("lib/hadoop-core-0.20.2.jar");
    addClassPath("lib/hsqldb-1.8.0.10.jar");
    addClassPath("lib/jasper-compiler-5.5.12.jar");
    addClassPath("lib/jasper-runtime-5.5.12.jar");
    addClassPath("lib/jets3t-0.7.1.jar");
    addClassPath("lib/jetty-6.1.14.jar");
    addClassPath("lib/jetty-util-6.1.14.jar");
    addClassPath("lib/jsp-2.1-6.1.14.jar");
    addClassPath("lib/jsp-api-2.1-6.1.14.jar");
    addClassPath("lib/junit-4.5.jar");
    addClassPath("lib/kfs-0.3.jar");
    addClassPath("lib/log4j-1.2.16.jar");
    addClassPath("lib/oro-2.0.8.jar");
    addClassPath("lib/servlet-api-2.5-6.1.14.jar");
    addClassPath("lib/xmlenc-0.52.jar");
    
    import java.util.*;
    
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.mapred.*;
    import org.apache.hadoop.conf.*;
    import org.apache.hadoop.io.*;
    import org.apache.hadoop.util.*;
    
    class Map extends MapReduceBase implements Mapper {
        one = new IntWritable(1);
        word = new Text();
    
        map(key, value, output, reporter) {
            line = value.toString();
            tokenizer = new StringTokenizer(line);
            while (tokenizer.hasMoreTokens()) {
                word.set(tokenizer.nextToken());
                output.collect(word, one);
            }
        }
    }
    
    class Reduce extends MapReduceBase implements Reducer {
        reduce(key, values, output, reporter) {
            int sum = 0;
            while (values.hasNext()) {
                sum += value.next().get();
            }
            output.collect(key, new IntWritable(sum));
        }
    }
    
    conf = new JobConf();
    conf.setJobName("wordcount");
    conf.setOutputKeyClass(Text.class);
    conf.setOutputValueClass(IntWritable.class);
    
    conf.setMapperClass(Map.class);
    conf.setCombinerClass(Reduce.class);
    conf.setReducerClass(Reduce.class);
    
    conf.setInputFormat(TextInputFormat.class);
    conf.setOutputFormat(TextOutputFormat.class);
    
    FileInputFormat.setInputPaths(conf, new Path("in"));
    FileInputFormat.setOutputPath(conf, new Path("out"));
    
    JobClient.runJob(conf);
    
  #+END_SRC
* TODO Analogy with cascalag-checkpoint
  From Aaron:

  #+BEGIN_QUOTE
  Props to Chun for pointing this out. Has some striking parallels to
  some of our requirements, so maybe a great source of inspiration for
  syntax. http://sritchie.github.com/2011/11/15/introducing-cascalogcontrib.html
                                                                                                                                                                                                                                                                                                                                                                                                                                         
  Notice for example there's an implicit naming convention for
  specifying sub tasks that run in parallel, vs. in series.
  #+END_QUOTE

  I do like the symbolic temporary directories; Vineyard's going to
  have to reap them appropriately, though.

  Rebind =read= in someone else's namespace?
* TODO [[http://hadoop.apache.org/common/docs/stable/mapred_tutorial.html][MapReduce tutorial]]
  When loading data, load into =DistributedCache=; [[https://github.com/stuartsierra/clojure-hadoop][clojure-hadoop]].
* TODO Attach-geocode POC
  #+BEGIN_EXAMPLE
    geocode-data(i3): /apps/extract/poi/UnitedKingdomScarecrow/input/geocode_data
    md5-uuid-mapping(i2): /apps/extract/poi/UnitedKingdomScarecrow/output/split_test/014_compute_uuids
    deduped_entities(i1): /apps/extract/poi/UnitedKingdomScarecrow/output/split_test/020_combined_deduped_and_validation_data
  #+END_EXAMPLE

  I.e. [[http://d22.factual.com.:50075/browseDirectory.jsp?dir=%2Fapps%2Fextract%2Fpoi%2FUnitedKingdomScarecrow%2Finput%2Fgeocode_data&namenodeInfoPort=50070&delegation=null][geocode_data]], [[http://d22.factual.com.:50075/browseDirectory.jsp?dir=%2Fapps%2Fextract%2Fpoi%2FUnitedKingdomScarecrow%2Foutput%2Fsplit_test%2F014_compute_uuids&namenodeInfoPort=50070&delegation=null][compute_uuids]], [[http://d22.factual.com.:50075/browseDirectory.jsp?dir=%2Fapps%2Fextract%2Fpoi%2FUnitedKingdomScarecrow%2Foutput%2Fsplit_test%2F022_combined_deduped_and_validation_and_geocoding_data&namenodeInfoPort=50070&delegation=null][combined_deduped_and_validation_data]].

  [[https://github.com/Factual/back/blob/master/datastore-objects/src/main/thrift/factual_data_objects.thrift][Thrift-spec]]: payload and payloadRaw are JSON; [[https://github.com/Factual/back/blob/master/datastore-objects/src/main/java/com/factual/adaptors/Input.java][Wraps the input-data
  object]]: i.e. parses the JSON, provides a map.

  "Attach": append it to the array data-objects; eventually:
  summarization merges the array of data-objects. Rules: mode, mean;
  more complex rules, e.g. this came from that source and has a higher
  score.

  Tab-delimited data; sequence files: key-type, value-type.

  uuid -> data; md5 -> uuid; md5 -> geodata; [[http://d11.factual.com:50075/browseBlock.jsp?blockId=-901183859042176514&blockSize=30109191&genstamp=13911775&filename=%2Fapps%2Fextract%2Fpoi%2FUnitedKingdomScarecrow%2Finput%2Fgeocode_data%2Fgeocode_data_2010_07_24&datanodePort=50010&namenodeInfoPort=50070&delegation=null][geodata]]. Represent the
  data as JSON; propagate geodata back.

  Generalized attachment: join, attach, summarize.

  - Task 1 (non-Hadoop)
    - Copy data to HDFS: comma-delimited md5-i -> uuid-i.
    - Input :: local file
    - Output :: md5->uuid (comma delimited)
  - Task 2 (cascalog?)
    - Convert output of task 1 to tab-delimited md5 -> uuid-i.
    - Input :: md5->uuid (comma delimited)
    - Output :: md5->uuid (tab delimited)
  - Task 3 (straight-up Java?)
    - Join task 2 with md5-i -> data-i, such that uuid-i -> data-i
      (tab delimited).
    - Input :: md5->uuid (tab delimited)
    - Output :: uuid->data (tab delimited)

  Simulates: non-Hadoop, Hadoop-transformation,
  Hadoop-join. (Cf. [[http://hadoop.apache.org/common/docs/stable/mapred_tutorial.html][this]], by the way.)

  #+BEGIN_SRC sh :tangle make-md5-to-uuid.sh :shebang #!/usr/bin/env bash
    n=${@:-100}
    
    for ((i = 0; i < n; i++)); do
        echo $(echo -n $i | openssl md5 | cut -d ' ' -f 2),$i
    done > md5-to-uuid
    
  #+END_SRC

  #+BEGIN_SRC clojure :tangle md5-uuid.clj :shebang #!/usr/bin/env clj
    (use 'add-classpath.core)
    (add-classpath "lib/*")
    (add-classpath "lib/dev/*")
    (use 'debug.core)
    (use 'clojure.java.io)
    (use 'clojure.string)
    
    (doseq [line (line-seq (reader "md5-uuid.txt"))]
      (let [[md5 uuid] (split line #",")]
        (println (format "%s\t%s" md5 uuid))))
    
  #+END_SRC

  #+BEGIN_SRC clojure :tangle uuid-data.clj :shebang #!/usr/bin/env clj
    (use 'add-classpath.core)
    (add-classpath "lib/*")
    (add-classpath "lib/dev/*")
    (use 'debug.core)
    (use 'clojure.java.io)
    (use 'clojure.string)
    (use 'clojure.data.json)
    
    (doseq [line (line-seq (reader *in*))]
      (let [[md5 uuid] (split line #"\t")]
        (println (format "%s\t%s" uuid (json-str {:uuid uuid})))))
    
  #+END_SRC

  #+BEGIN_SRC java :tangle UUIDToMD5Mapper.java
    import java.io.IOException;
    
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.mapred.*;
    import org.apache.hadoop.mapred.lib.*;
    import org.apache.hadoop.conf.*;
    import org.apache.hadoop.io.*;
    import org.apache.hadoop.util.*;
    
    public class UUIDToMD5Mapper {
    
        public static class Map extends MapReduceBase
            implements Mapper<LongWritable, Text, Text, Text> {
            public void map(LongWritable key,
                            Text MD5ToUUID,
                            OutputCollector<Text, Text> output,
                            Reporter reporter)
                throws IOException {
                String[] MD5AndUUID = MD5ToUUID.toString().split(",");
                String MD5 = MD5AndUUID[0];
                String UUID = MD5AndUUID[1];
                output.collect(new Text(UUID),
                               new Text(String.format("md5: %s", MD5)));
            }
        }
    
        public static void main(String[] argv) throws IOException {
            JobConf conf = new JobConf(UUIDToMD5Mapper.class);
            conf.setJobName("map-uuid-to-md5");
            conf.setOutputKeyClass(Text.class);
            conf.setOutputValueClass(Text.class);
            conf.setMapperClass(Map.class);
            conf.setReducerClass(IdentityReducer.class);
            conf.setInputFormat(TextInputFormat.class);
            // conf.setOutputFormat(TextOutputFormat.class);
            conf.setOutputFormat(SequenceFileOutputFormat.class);
            FileInputFormat.setInputPaths(conf, new Path("md5-to-uuid"));
            FileOutputFormat.setOutputPath(conf, new Path("uuid-to-md5"));
            JobClient.runJob(conf);
        }
    }
    
  #+END_SRC

  #+BEGIN_SRC sh :tangle map-uuid-to-md5.sh :shebang #!/usr/bin/env bash
    org-tangle TODO.org && \
        rm -frv uuid-to-md5 && \
        mkdir -v UUIDToMD5Mapper-classes;
    
    javac -cp $(hadoop classpath) -d UUIDToMD5Mapper-classes UUIDToMD5Mapper.java && \
        jar -cvf UUIDToMD5Mapper.jar -C UUIDToMD5Mapper-classes . && \
        hadoop jar UUIDToMD5Mapper.jar UUIDToMD5Mapper && \
        hadoop fs -cat uuid-to-md5/*
    
  #+END_SRC

  #+BEGIN_SRC java :tangle UUIDToPayloadMapper.java
    import java.io.IOException;
    
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.mapred.*;
    import org.apache.hadoop.mapred.lib.*;
    import org.apache.hadoop.conf.*;
    import org.apache.hadoop.io.*;
    import org.apache.hadoop.util.*;
    
    public class UUIDToPayloadMapper {
    
        public static class Map extends MapReduceBase
            implements Mapper<LongWritable, Text, Text, Text> {
            public void map(LongWritable key,
                            Text MD5ToUUID,
                            OutputCollector<Text, Text> output,
                            Reporter reporter)
                throws IOException {
                String[] MD5AndUUID = MD5ToUUID.toString().split(",");
                String MD5 = MD5AndUUID[0];
                String UUID = MD5AndUUID[1];
                output.collect(new Text(UUID),
                               new Text(String.format("time: %s",
                                                      System.currentTimeMillis())));
            }
        }
    
        public static void main(String[] argv) throws IOException {
            JobConf conf = new JobConf(UUIDToPayloadMapper.class);
            conf.setJobName("map-uuid-to-payload");
            conf.setOutputKeyClass(Text.class);
            conf.setOutputValueClass(Text.class);
            conf.setMapperClass(Map.class);
            conf.setReducerClass(IdentityReducer.class);
            conf.setInputFormat(TextInputFormat.class);
            // conf.setOutputFormat(TextOutputFormat.class);
            conf.setOutputFormat(SequenceFileOutputFormat.class);
            FileInputFormat.setInputPaths(conf, new Path("md5-to-uuid"));
            FileOutputFormat.setOutputPath(conf, new Path("uuid-to-payload"));
            JobClient.runJob(conf);
        }
    }
    
  #+END_SRC

  #+BEGIN_SRC sh :tangle map-uuid-to-payload.sh :shebang #!/usr/bin/env bash
    org-tangle TODO.org && \
        rm -frv uuid-to-payload && \
        mkdir -v UUIDToPayloadMapper-classes;
    
    javac -cp $(hadoop classpath) -d UUIDToPayloadMapper-classes UUIDToPayloadMapper.java && \
        jar -cvf UUIDToPayloadMapper.jar -C UUIDToPayloadMapper-classes . && \
        hadoop jar UUIDToPayloadMapper.jar UUIDToPayloadMapper && \
        hadoop fs -cat uuid-to-payload/*
    
  #+END_SRC

  Swap it: "UUID\tMD5" after the first job; input to the second job:
  tab-delimited values and the simulated payload; when reducing during
  the second job, should see UUID -> (md5, payload)?

  Output of second job: combine the md5 and payload (i.e. insert md5
  into payload).

  Using sequence-files instead of text-files should give me key-value
  pairs (and obviate the need for destructuring the tab).

  #+BEGIN_SRC java :tangle MD5AndPayloadJoiner.java
    import java.io.IOException;
    import java.util.*;
    
    import org.apache.commons.logging.Log;
    import org.apache.commons.logging.LogFactory;
    
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.mapred.*;
    import org.apache.hadoop.mapred.lib.*;
    import org.apache.hadoop.conf.*;
    import org.apache.hadoop.io.*;
    import org.apache.hadoop.util.*;
    
    public class MD5AndPayloadJoiner {
        public static class Reduce extends MapReduceBase
            implements Reducer<Text, Text, Text, Text> {
            public void reduce(Text UUID,
                               Iterator<Text> values,
                               OutputCollector<Text, Text> output,
                               Reporter reporter)
                throws IOException {
                StringBuilder data = new StringBuilder();
                while (values.hasNext()) {
                    data.append(String.format("%s ", values.next()));
                }
                output.collect(UUID, new Text(data.toString()));
            }
        }
    
        public static void main(String[] argv) throws IOException {
            JobConf conf = new JobConf(MD5AndPayloadJoiner.class);
            conf.setJobName("map-uuid-to-payload");
            conf.setOutputKeyClass(Text.class);
            conf.setOutputValueClass(Text.class);
            conf.setMapperClass(IdentityMapper.class);
            conf.setReducerClass(Reduce.class);
            conf.setInputFormat(SequenceFileInputFormat.class);
            conf.setOutputFormat(TextOutputFormat.class);
            // conf.setOutputFormat(SequenceFileOutputFormat.class);
            FileInputFormat.addInputPath(conf, new Path("uuid-to-md5"));
            FileInputFormat.addInputPath(conf, new Path("uuid-to-payload"));
            FileOutputFormat.setOutputPath(conf, new Path("md5-and-payload"));
            JobClient.runJob(conf);
        }
    }
    
  #+END_SRC

  #+BEGIN_SRC sh :tangle join-md5-and-payload.sh :shebang #!/usr/bin/env bash
    org-tangle TODO.org && \
        rm -frv md5-and-payload && \
        mkdir -v MD5AndPayloadJoiner-classes;
    
    javac -cp $(hadoop classpath):classes -d MD5AndPayloadJoiner-classes MD5AndPayloadJoiner.java && \
        jar -cvf MD5AndPayloadJoiner.jar -C MD5AndPayloadJoiner-classes . && \
        hadoop jar MD5AndPayloadJoiner.jar MD5AndPayloadJoiner && \
        hadoop fs -cat md5-and-payload/*
    
  #+END_SRC

  - https://github.com/Factual/vineyard/blob/master/hadoop/src/main/java/vineyard/hadoop/demojob/WordCounter.java
  - https://github.com/Factual/vineyard/blob/master/hadoop/pom.xml
  - http://wiki.corp.factual.com/display/ENG/Internal+Maven+Proxy+Repository
  - https://github.com/Factual/vineyard/blob/master/hadoop/src/test/java/vineyard/hadoop/Producer.java

* TODO Spec
  If we have:

  #+BEGIN_SRC clojure
    (deftask b
      :children (c d e)
      :dependencies (a))
  #+END_SRC

  I also want:

  #+BEGIN_SRC clojure
    (deftask a ...)
    (deftask b ...)
    (deftask c ...)
    ...
    
    (make-task-tree!
     (a
      (b
       (c d e))))
    
    (make-dependency-tree!
     (a
      (b)))
  #+END_SRC

  where tasks are created with the default settings, if they don't
  exist; possibly with a warning on stdout.
* TODO Proof-of-concept
  Chain two map-reduce tasks together. Vineyardize the tasks (without
  napa).

  Bogus wordcount example?

  Capitalize, count.

  Over hadoop.

  Non-hadoop precondition: moves local file with noisy words to HDFS;
  in hadoop: normalization (upper-case) and count.

  Output: word to count mapping:

  #+BEGIN_EXAMPLE
    ASS 1
    DONKEY 10
  #+END_EXAMPLE

  Validation: validating counters (name of counter, value),
  hdfs-file-exists?, hdfs-file-empty?

  #+BEGIN_SRC clojure
    (defn hadoop-counter [counter-name]
      ...)
    
    (defn call-with-hadoop-conditions [f]
      (f *hadoop-conditions*))
    
    (defn non-zero-hadoop-conditions? []
      (call-with-hadoop-conditions
        (fn [hadoop-conditions]
          (> (count hadoop-conditions) 0))))
    
    (if (non-zero-hadoop-conditions?)
      (throw ...Exception))
    
    (defn get-hadoop-job [vineyard-task]
      (...))
    
    (defn get-hadoop-counter [vineyard-task counter-name]
      (...))
    
    (defn get-hadoop-property [vineyard-ask property-name]
      (...))
    
    (> (get-hadoop-counter *vineyard-task* "foo") 0)
    
    (defn hdfs-file-exists? [vineyard-task path]
      ;; Check for the existence of _SUCCESS.
      (...))
    
    (hdfs-file-exists? *vineyard-task* "/path/to/dedupe")
    
    ;;; Inside pre-dedupe-analysis; path defaults to "/path/to/dedupe". In
    ;;; other words, "does the default input path of my parent exist?"
    (hdfs-file-parent-exists? *vineyard-task*)
    
  #+END_SRC

  [[http://wiki.corp.factual.com/display/INFRA/Vineyard+Java+Driver][Vineyard Java client]]. MapReduce jobs in Clojure? And pre-existing
  code in Java.

* TODO Extract POC
  [[https://github.com/Factual/hadoop-extraction-workflow/blob/master/src/java/workflows/extract/poi/UnitedStatesExtraction.java][US-extraction]]; enumerated subtasks:

  #+BEGIN_SRC java
    List<mapreduce.Task> tasks =
        Lists.newArrayList
        (
         writeHeaders,
         computeUniqueInputs,
         convertGeocodingResultsToTab,
         convertValidationJsonResultsToTab,
         writeUuidRetentionMappingToSequenceFile,
         extractEntities,
         computeSortedUniqueMd5s,
         analyzeExtractedEntities,
         postProcessExtractedData,
         preDedupeAnalysis,
         preDedupeAnalysisSummary,
         generateLikelyDupeMd5s,
         uniquifyLikelyDupeMd5s,
         computeDedupeUuids,
         assignUuids,
         groupDedupedEntities,
         dedupeQA,
         removeJunkInputsAndEntities,
         assignUuidsToValidationResults,
         combineDedupedAndValidationData,
         assignUuidsToGeocodingResults,
         combineDedupedAndGeocodingData,
         performFinalPostprocessing,
    
         computeUuidRetentionMapping,
         removeOverfoldingRetainedUuids,
         applyUuidRetentionMapping,
    
         exportData,
         exportDataQA,
    
         uuidRetentionTracker
         );
    
  #+END_SRC

  [[http://d22.factual.com.:50075/browseDirectory.jsp?dir=%2Fapps%2Fextract%2Fpoi%2FUnitedKingdomScarecrow%2Foutput%2Fleo_uuid_test&namenodeInfoPort=50070&delegation=null][Output]].

  #+BEGIN_SRC sh
    sudo hadoop jar hadoop-extraction-workflow-hadoop.jar \
        workflows.extract.poi.UnitedKingdomExtraction \
        hadoop_config_file=conf/mapreduce/MapReduceRunner/n_cluster.properties \
        extraction_config_class=extract.poi.UnitedKingdomScarecrow \
        project_name=UK_scarecrow_extraction_test \
        extraction_dataset_id=G4YzkQ \
        summary_view_id=cZqm0N
  #+END_SRC

  Modules:

  - Extract
  - Dedupe
  - Attach geo
  - Attach validation
  - UUID retention

  #+BEGIN_SRC lisp
    ;;; Grouping
    (hadoop-extraction-workflow
     (extract
      ^{predecessors: (extract)}
      (write-headers
       compute-unique-inputs
       convert-geocoding-results-to-tab
       convert-validation-json-results-to-tab
       write-uuid-retention-mapping-to-sequence-file
       extract-entities
       compute-sorted-unique-md5s
       analyze-extracted-entities
       post-process-extracted-data))
     (dedupe
      (pre-dedupe-analysis
       pre-dedupe-analysis-summary
       generate-likely-dupe-md5s
       uniquify-likely-dupe-md5s
       compute-dedupe-uuids
       assign-uuids
       group-deduped-entities
       dedupe-qa
       remove-junk-inputs-and-entities
       perform-final-postprocessing))
     (attach-geo
      (assign-uuids-to-geocoding-results
       combine-deduped-and-geocoding-data))
     (attach-validation
      (assign-uuids-to-validation-results
       combine-dedupe-and-validation-data))
     (uuid-retention
      (compute-uuid-retention-mapping
       remove-overfolding-retained-uuids
       apply-uuid-retention-mapping
       export-data
       export-data-qa
       uuid-retention-tracker)))
    
    ;;; Precedence
    (dedupe (extract))
    
    ;;; Gantt charts
    
    (a
     (b
      (c d)))
    
  #+END_SRC

* TODO =yaml= to vineyard
  We're going to have a =.onStart=, =.onFinish=; yaml leaves specify
  tasks. Have a predecessor thing:

  #+BEGIN_EXAMPLE
    iris
      chrome plugin
      nlp
    api
      places data
      sugar
    demo
      webui (depends 1, 4)
  #+END_EXAMPLE
* Notes
** Mon Dec 19 16:50:50 PST 2011   
   - at the end of task: check succeeded (Vineyard task); it fails;
     responsibility of the vineyard task to fail;
   - takes YAML: turns into command-line options
   - napa is the consumer that can run in daemon or cli mode (latter:
     takes yaml file, presents
   - name of the yaml file, config-argument
   - folders of yaml files
   - yaml files exist in scarecrow?
   - yaml files served up by screws?
   - composition of yaml-files?
   - "this step is actually this file"

** Tue Dec 27 09:56:33 PST 2011
   - command-line stuff: automatically parse the yaml: populate
     command line opts
   - workflow definition language
